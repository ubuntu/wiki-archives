{|
  | '''Warning'''
  * This is a '''readonly''' and '''text-based''' archive of a deprecated wiki.
  * '''Images''' and '''attachments''' have been removed to conserve space.
  * '''Links''' may not work and there may be formatting issues.
  * A '''compressed''' version with images and the original syntax is in the repo '''Releases'''.
|}

__TOC__



== Introduction ==

RAID is a method of using multiple hard drives to act as one. There are two purposes of RAID:
** Expand drive capacity: RAID 0. If you have 2 x 500 GB HDD then total space become 1 TB.
** Prevent data loss in case of drive failure: For example RAID 1, RAID 5, RAID 6, and RAID 10.

There are three ways to create a RAID:
1. Software-RAID: Where the RAID is created by software.
2. Hardware-RAID: A special controller used to build RAID. Hardware RAID is generally faster, and does not place load on the CPU, and hardware RAID can be used with any OS
3. FakeRAID: Since RAID hardware is very expensive, many motherboard manufacturers use multi-channel controllers with special BIOS features to perform RAID. This is a form of software RAID using special drivers, and it is not necessarily faster than true software RAID. Read <nowiki>[[FakeRaidHowto]]</nowiki> for details.

== Requirements ==

** If you're building a server, the server install ISO includes the necessary options.
** If you're building a desktop then you need the "Alternate" install ISO for Ubuntu. Read <nowiki>[[GettingUbuntu#head-40b5bcdbc1d4ec7b8149519dfd4f08c7fa274559| Getting Ubuntu Alternate Install disk]]</nowiki> and <nowiki>[[Installation#head-194b248381c71c37f7b187c6b814bbe7e31d91d6| How to do a Ubuntu Alternate Install]]</nowiki>
** <nowiki>[[BurningIsoHowto|How to Burn an ISO]]</nowiki>
** Enough drives to meet the requirements of the RAID.

== Installing via the GUI ==

Install Ubuntu until you get to partitioning the disks

<nowiki>[[File:ubuntu_raid_00.png]]</nowiki> 

=== Partitioning the disk ===

'''Warning: the /boot filesystem cannot use any softRAID level other than 1''' with the stock Ubuntu bootloader. If you want to use some other RAID level for most things, you'll need to create separate partitions and make a RAID1 device for /boot.

'''Warning: this will remove all data on hard drives.''' 

1. Select "Manual" as your partition method 

<nowiki>[[File:ubuntu_raid_01.png]]</nowiki>

2. Select your hard drive, and agree to "Create a new empty partition table on this device ?" 

<nowiki>[[File:ubuntu_raid_02.png]]</nowiki> <nowiki>[[File:ubuntu_raid_03.png]]</nowiki>

3. Select the "FREE SPACE" on the 1st drive then select "automatically partition the free space 

<nowiki>[[File:ubuntu_raid_04.png]]</nowiki> <nowiki>[[File:ubuntu_raid_05.png]]</nowiki>

4. Ubuntu will create 2 partitions: / and swap, as shown below: 

<nowiki>[[File:ubuntu_raid_06.png]]</nowiki>

5. On / partition select "bootable flag" and set it to "on"

<nowiki>[[File:ubuntu_raid_06.png]]</nowiki>

6. Repeat steps 2 to 5 for the other hard drive

== Configuring the RAID ==

1. Once you have completed your partitioning in the main "Partition Disks" page select "Configure Software RAID"
2. Select "Yes"
3. Select "Create new MD drive"
4. Select RAID type: RAID 0, RAID 1, RAID 5 or RAID 6
5. Number of devices. RAID 0 and 1 need 2 drives. 3 for RAID 5 and 4 for RAID 6.
6. Number of spare devices. Enter 0 if you have no spare drive.
7. select which partitions to use..
8. Repeat steps 3 to 7 with each pair of partitions you have created.
9. Filesystem and mount points will need to be specified for each RAID device. By default they are set to "do not use".
10. Once done, select finish. 

== Boot Loader ==

In case your next HDD won't boot then simply install Grub to another drive:
<pre>
grub-install /dev/sdb
</pre>
<pre>
grub-install /dev/sdc 
</pre>

== Boot from Degraded Disk ==

If the default HDD fails then RAID will ask you to boot from a degraded disk. If your server is located in a remote area, the best practice may be to configure this to occur automatically: 

1. edit /etc/initramfs-tools/conf.d/mdadm
2. change "BOOT_DEGRADED=false" to "BOOT_DEGRADED=true"
* Additionally, this can be specified on the kernel boot line with the bootdegraded=[true|false]
* You also can use #dpkg-reconfigure mdadm rather than CLI! 

== Verify the RAID ==

1. shut-down your server
2. remove the power and cable data of your first drive
3. start your server and see if your server can boot from a degraded disk.

== Troubleshooting ==

== Swap space doesn't come up, error message in dmesg ==

Provided the RAID is working fine this can be fixed with:
<pre>
sudo update-initramfs -k all -u
</pre>

== Using the mdadm CLI ==

For those that want full control over the RAID configuration, the mdadm CLI provides this.

== Checking the status of your RAID ==

Two useful commands to check the status are:
<pre>
cat /proc/mdstat
</pre>
Example output:
<pre>
 Personalities : [raid1] [raid6] [raid5] [raid4]
md5 : active raid1 sda7[0] sdb7[1]
      62685504 blocks [2/2] [UU]

md0 : active raid1 sda1[0] sdb1[1]
      256896 blocks [2/2] [UU]

md6 : active raid5 sdc1[0] sde1[2] sdd1[1]
      976767872 blocks level 5, 64k chunk, algorithm 2 [3/3] [UUU]
</pre>
From this information you can see that the available personalities on this machine are "raid1, raid6, raid4, and raid5" which means this machine is set-up to use raid devices configured in a raid1, raid6, raid4 and raid5 configuration.

You can also see in the three example meta devices that there are two raid 1 mirrored meta devices. These are md0 and md5. You can see that md5 is a raid1 array and made up of disk /dev/sda partition 7, and /dev/sdb partition 7, containing 62685504 blocks, with 2 out of 2 disks available and both in sync.

The same can be said of md0 only it is smaller (you can see from the blocks parameter) and is made up of /dev/sda1 and /dev/sdb1.

md6 is different in that we can see it is a raid 5 array, striped across three disks. These are /dev/sdc1, /dev/sde1 and /dev/sdd1, with a 64k "chunk" size or write size. Algorithm 2 shows it is a write algorithm pattern, which is "left disk to right disk" writing across the array. You can see that all three disks are present and in sync.
<pre>
sudo mdadm --query --detail /dev/md* 
</pre>
Replace * with the partition number.

== Disk Array Operation ==

Note: You can add, remove disks, or set them as faulty without stopping an array.

1. To stop an array, type:
<pre>
sudo mdadm --stop /dev/md0
</pre> Where /dev/md0 is the array device.

2. To remove a disk from an array:
<pre>
sudo mdadm --remove /dev/md0 /dev/sda1
</pre> Where /dev/md0 is the array device and /dev/sda is the faulty disk.

3. Add a disk to an array:
<pre>
sudo mdadm --add /dev/md0 /dev/sda1
</pre> Where /dev/md0 is the array device and /dev/sda is the new disk.
'''Note''': This is not the same as "growing" the array!

4. Start an Array, to reassemble (start) an array that was previously created:
<pre>
mdadm --assemble --scan
</pre> ddadm will scan for defined arrays and start assembling it. 

5. To track the status of the array as it gets started:
<pre>
cat /proc/mdstat
</pre>

== Known bugs ==

Ubuntu releases starting with 12.04 does not support nested raids like levels 1+0 or 5+0 due to an unresolved issue https://bugs.launchpad.net/ubuntu/+source/mdadm/+bug/1171945

=== Resources ===

* http://manpages.ubuntu.com/manpages/trusty/man8/mdadm.8.html
* https://wiki.ubuntu.com/HotplugRaid Keeping your data synced and mirrored on external drives.
* http://en.wikipedia.org/wiki/RAID
* http://en.wikipedia.org/wiki/Mdadm 
