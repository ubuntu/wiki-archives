{|
| '''Warning'''
* This is a '''readonly''' and '''text-based''' archive of a deprecated wiki.
* '''Images''' and '''attachments''' have been removed to conserve space.
* '''Links''' may not work and there may be formatting issues.
* A '''compressed''' version with images and the original syntax is in the repo '''Releases'''.
|}

__TOC__

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; background:#F1F1ED; margin: 0 0 1em 1em;" style="padding:0.5em;">
|}

=== Windows-to-Linux Roadmap: Overview ===
'''A roadmap for developers making the transition to Linux'''

Level: Introductory

Chris Walden (dwinfo@us.ibm.com), e-business Architect, IBM
11 Nov 2003

IBM e-business architect Chris Walden is your guide through a nine-part developerWorks series on
moving your operational skills from a Windows® to a Linux® environment. He covers everything from
logging to networking, and from the command-line to help systems -- even compiling packages from
available source code.

You're moving from Windows to Linux. You've decided you want the stability, flexibility, and cost savings of Linux,
but you have many questions in your head. Isn't Linux like UNIX? Isn't UNIX hard? Where do you begin to make
sense of all of this? Is there a map you can follow?

This roadmap is designed to help you take the experience and knowledge that you already have in computing and
redirect it to working in Linux. It's not the only reference you'll ever need, but it will help you get past some of your
first obstacles and adjust to a new and, I think, exciting approach to computing. As you follow this roadmap, you'll
discover many new resources to help you learn, troubleshoot, and manage Linux.

We're assuming you already have Linux installed. If you don't, go to Linux.org and learn which distributions would fit
your needs. You'll also find links to downloads there when you're ready to install.

Your roadmap to Linux:

{| class="wikitable"
|-
| Step 1. Thinking in Linux
| The first step to success in Linux is learning to think in Linux. Take what you already know and redirect it to doing things the Linux way.
|-
| Step 2. Console crash course
| Linux provides great power and flexibility through the console. If it has been a while since you've spent much time at the command prompt, take a little time to reacquaint yourself with this environment by reviewing common commands you'll use all the time.
|-
| Step 3.  Introduction to Webmin
| While it is important to know the nuts and bolts of administration, it is often more convenient to have a tool. Also, a higher-level application makes complex configurations easier to handle. Webmin provides point-and-click configuration for beginning and experienced administrators.
|-
| Step 4. User administration
| If a system has no users, is it really a system? Learn about the Linux approach to users.
|-
| Step 5. Linux logging
| Linux makes extensive use of logging. Nothing is hidden from you. Becoming comfortable and familiar with logs will allow you to monitor the health of your system and track activities.
|-
| Step 6. Working with file systems
| File systems are at the heart of every server. Linux provides a lot of flexibility in its file systems.
|-
| Step 7. Networking
| Working unconnected is unthinkable in today's world. Linux on the network unleashes its full potential.  However, Linux networking looks very different on its face. You'll need to learn some new terminologies and new tools.
|-
| Step 8. Backup and recovery
| The first line of defense against disaster is a backup of the data. Linux provides different options, some of which are very simple to work with.
|-
| Step 9. Installing software
| Linux can use prepackaged binary files, or you can compile programs directly from source code. The tools for installing Linux programs are very useful and provide functionality you might not expect.
|}

----

'''Resources'''

'''Learn'''
* More information on transitioning to Linux awaits you on the developerWorks New to Linux page.
* Find more resources for Linux developers in the developerWorks Linux zone, including our newest how-to tutorials.
* Hone your skills in Linux basics and systems administration with our certification exam study guides. Whether you choose to take the exams or not, our Linux skill-building tutorial series will immerse you in Linux fundamentals as well as advanced topics.
* Learn how to acquire kernel source, configure and boot your new kernel, add a feature, fix a flaw, or just have fun tinkering with operating system source code in our Hacking the Linux kernel tutorial series. Hack and be free.
* IBM developerWorks technical events and Webcasts are a great way to learn more about Linux as well as IBM products that run on Linux.
* The Linux at IBM site offers software, links, end-to-end Linux solutions, and more.
* The Linux Documentation Project is a repository of Linux documentation including documents about individual software, HOWTO documents, FAQs, and more.
* Linux Online! offers non-partisan Linux news and information.
* The O'Reilly Network is an excellent resource for technical books on Linux.

'''Get products and technologies'''
* Order the SEK for Linux, a two-DVD set containing the latest IBM trial software for Linux from DB2®, Lotus®, Rational®, Tivoli®, and WebSphere®.
* Build your next development project on Linux with IBM trial software, available for download directly from developerWorks.

'''Discuss'''
* Participate in the discussion forum.
* Read developerWorks blogs, and get involved in the developerWorks community.

=== Windows-to-Linux roadmap: Part 1. Thinking in Linux ===
'''Differences and similarities'''

You have started to make the change from Windows to Linux. This may have been your decision, or may have come
from "on high." In either case you are looking at the prospect of changing your procedures and tools from something
very familiar to something perhaps completely unknown. Furthermore, you may need to train others on how to
comfortably manage Linux. Administration is more than just following procedures. There is a creative side to
successfully managing a computing environment. Making this transition can feel like changing from being an
accomplished oil painter to doing welding sculpture.

The good news is that the open standards that drive Linux are the same standards that formed the foundation for your
Windows administration. Some of the buzzwords are different, and the tools are a little different, but the underlying
concepts are the same. The better news is that as you become accustomed to the Linux approach to doing things, you
will have an even larger toolset to work with to control and maintain your environment. Linux's strength is in its
stability and flexibility. As you learn to think in Linux, you'll accomplish more, automate more, and do more remotely.

'''Linux is a little like Windows'''

Before discussing how Linux is different from Windows, let's explore the overlaps where Linux and Windows are
similar.

'''Users and groups'''

Both Linux and Windows are multi-user operating systems. Both can be used by many different users, and give each
user a separate environment and resources. Security is controlled based on the user's identity. Resource access can also
be controlled by group membership, making it easier to work with rights for large numbers of users without having to
touch each individual account.

Users and groups can be centralized into a single repository, allowing multiple servers to share the same user and
authentication data.

'''File system'''

Both Linux and Windows can work with a wide variety of file systems. File resources can be shared with a variety of
clients through NetBIOS, FTP, or other protocols. Individual file systems can be flexibly incorporated, allowing the
administrator to choose where and how they will be accessed.

'''Ports and devices'''

Physical device ports such as parallel, serial, and USB are supported. Various controllers, such as IDE and SCSI, are
also supported. Linux can support a good deal of standard hardware "off the shelf."

'''Networking'''

Linux and Windows both support a number of networking protocols, such as TCP/IP, NetBIOS, and IPX. Both support
a wide variety of network adapters. Both provide the ability to share resources, such as files and printing, through the
network. Both provide capability to perform network services, such as DHCP and DNS.

'''Services'''

Linux and Windows both have services, applications that run in the background to provide some function to the
system and to computers that remotely call the service. These programs can be controlled individually and be started
automatically when the system boots. (Note: In Linux these applications are often referred to as ''daemons'', a legacy of
its Unix roots.)

'''Linux is different from Windows'''

Even though there are several similarities between the technologies, there are some key differences between the style
of working in Windows and working in Linux. These differences are subtle until you get used to them, but they are
key concepts to thinking in Linux.

Linux is built for the network more than printing
When Windows was first built, it was mostly a paper world. One of the beautiful things about Windows was that
everything you did was pretty to look at and easy to print. This beginning has affected the evolution of Windows.
In the same manner, Linux has been affected by its origins. Linux was designed from the beginning to live on the
network. It was inspired by the Unix operating system, so there was a simplicity, some might say terseness, to its
command design. Since plain text works well across a network, text has always been the base for Linux configuration
and data.

For those accustomed to a graphical environment, a Linux server might appear primitive at first glance. But Linux
development has focused for most of its life more on what was under the hood. Linux has very sophisticated
networking, scripting, and security capabilities that are active even in a text-only environment. Some of the seemingly
bizarre steps required to perform some tasks are inexplicable until you realize that Linux expects to perform these tasks
on a network in cooperation with other Linux systems. Linux has very capable automation capabilities and can be
programmed to perform very detailed tasks using nothing more than the equivalent of batch file programming. Linux's
text-based nature is a large part of this capability.

'''Optional GUI'''

Linux has a graphical component. Linux is capable of working with high-end graphical adapters and displays to do
some truly stunning work. In fact, many digital effects artists do their design work on Linux workstations, where they
would have used IRIX systems in the past. However, the graphical environment is not integral to Linux. It is a layer on
top of the running system. That means that you run the GUI only if and when you need it. If your system spends most
of its time serving up Web applications, then you can turn off the overhead of the graphical interface and use that
memory and CPU for your service. If you need to do work on the system in a GUI, you can turn it on for your work
and turn it off when you're done.

There are graphical tools for managing Linux, as well as tools for general office work, such as e-mail, Web-browsing,
and document processing. However, in Linux, the graphical administration tools are normally front ends for the
console (command-line) tools. That means that anything you can do with a graphical tool, you can also do with a
console command. Also, using a graphical tool doesn't prevent you from making manual changes to a configuration
file. The value of this may not be immediately apparent. But think about it. If anything done from a graphical
administration tool can be done with a console command, that means that those tasks can also be scripted. Scripted
commands can become automated tasks. Linux provides the best of both worlds and doesn't force you to work with
only text or GUI. You choose the method that's best for your task.

Configuration files in Linux are human-readable text files. This is similar to the INI file of Windows past. This is a
philosophical difference from the Windows Registry approach. Configuration files are generally provided for
individual applications, and they are usually kept isolated from other configurations. However, most configuration
files live in a single place on the directory tree (/etc), so there is a logical single place to look. Text file configuration
makes it easy to back up, examine, and edit configurations without using any special system tools.

'''Filename extensions'''

Linux does not use filename extensions to identity a file's type. Rather, Linux looks at the header contents of a file to
identity the type. You can still use filename extensions for human-readability, but Linux doesn't care. That said, some
applications, such as a Web server, may use naming conventions to identify file types, but that is a factor of the
individual applications.

Linux uses file access rights to determine if a file is an executable. Any file can be given executable status, so
programs and scripts can be identified as executable by the creator or administrator. One clear advantage to this is
security. An executable file saved onto the system cannot necessarily be automatically executed, a feature that thwarts
many script viruses.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; background:#F1F1ED; margin: 0 0 1em 1em;" style="padding:0.5em;">'''What's the kernel?'''<<BR>>Linux is actually just the kernel; it implements multitasking and multiuser functionality, manages hardware, allocates memory, and enables applications to run.<<BR>>For the beginner, probably the most important thing about the kernel that you need to remember is that odd-numbered kernel versions (in other words, 2.3, 2.5, 2.7) are the experimental, development kernel.  Stable, release kernels carry even numbers (in other words, 2.4, 2.6, 2.8).
|}

'''Rebooting is a last resort'''

If you have been using Windows for a long time, you are accustomed
to rebooting the system for many reasons, from software installation to
correcting problems with a service. This is a habit you will need to
change to start thinking in Linux. Linux tends to be rather Newtonian
in nature. Once set in motion, it will tend to stay in motion until it is
acted upon by an outside force, such as a hardware failure. In fact, the
system design of Linux prevents applications from corrupting the
kernel, which is why it doesn't need frequent reboots (in contrast to
Windows system design). So except for the Linux kernel, you can
install, start, stop, and reconfigure software without having to reboot
the system.

If you do reboot your Linux system, it will not likely change the
behavior, and can make a problem worse. Learning to work with the Linux services and run levels is key to successful
troubleshooting. Of everything you will learn going into Linux, overcoming the reboot habit will probably be the
hardest.

However, the good news is that this allows you to do a lot of work remotely in Linux. As long as some basic network
services are running, you can probably get into the system. Also, if you are having problems with a particular service
on a system, you can let the others run while you continue to troubleshoot the problem. When you are consolidating
several services onto a single system, this is a critical difference.

'''Commands are case sensitive'''

All Linux commands and options are case sensitive. For example, '''-R''' is different from '''-r''', and will do different things.
Console commands are almost always lowercase. We'll cover commands in more detail in "Part 2. Console crash
course."

----

'''What should I be thinking about Linux?'''

The transition from administering Windows to Linux is not trivial. However, as a Windows administrator, you already
have many advantages. Much of your understanding of how computing works will carry over. Success as a Linux
administrator will be a matter of identifying the differences and adjusting your habits.
Many of the differences between Linux and Windows are advantageous. Overhead from an idle GUI can be reclaimed for services. Tasks can be scripted and automated. Configuration files are text based and human editable. You do not
have to reboot the system for most tasks. In fact, you should overcome your instinct to reboot.

----

'''Resources'''

'''Learn'''
* Read the other installments in this Windows-to-Linux roadmap (developerWorks, November 2003).
* More information on transitioning to Linux awaits you on the developerWorks New to Linux page.
* Find more resources for Linux developers in the developerWorks Linux zone, including our newest how-to tutorials.
* Hone your skills in Linux basics and systems administration with our certification exam study guides. Whether you choose to take the exams or not, our Linux skill-building tutorial series will immerse you in Linux fundamentals as well as advanced topics.
* Learn how to acquire kernel source, configure and boot your new kernel, add a feature, fix a flaw, or just have fun tinkering with operating system source code in our Hacking the Linux kernel tutorial series. Hack and be free.
* IBM developerWorks technical events and Webcasts are a great way to learn more about Linux as well as IBM products that run on Linux.
* The Linux at IBM site offers software, links, end-to-end Linux solutions, and more.
* The Linux Documentation Project is a repository of Linux documentation including documents about individual software, HOWTO documents, FAQs, and more.
* Linux Online! offers non-partisan Linux news and information.
* The O'Reilly Network is an excellent resource for technical books on Linux.

'''Get products and technologies'''
* Order the SEK for Linux, a two-DVD set containing the latest IBM trial software for Linux from DB2®, Lotus®, Rational®, Tivoli®, and WebSphere®.
* Build your next development project on Linux with IBM trial software, available for download directly from developerWorks.

'''Discuss'''
* Participate in the discussion forum.
* Get involved in the developerWorks community by participating in developerWorks blogs.

=== Windows-to-Linux roadmap: Part 2. Console crash course ===
'''A quick guide to the Linux console'''

In this part, we cover the different shells, as well as some of the most essential Linux commands.

Any administrative task can be done from the console in Linux. In many cases, using the console is faster than using a
graphical program and may provide additional functionality. Furthermore, any console task can be placed into a script,
and thus automated. To really take control of your Linux environment, you will want to learn how to do things from
the console. The information here is a guide to get you started at the Linux console if you have a DOS/Windows
background.

'''Accessing a console'''

If your system boots into text mode (a common configuration for servers to conserve overhead for services), then you
are already at a console when you execute a text login. On a typical Linux system, you can get to additional consoles
by pressing Ctrl + Alt + (F1 - F6). Each console is a completely different session on the system and can be accessed as
different users at the same time.

This multi-console behavior is different from the multiple-desktop in Windows. In Linux, each console can be
controlled by a completely different user. For example, you can be logged as ''root'' on console 1, and logged in as
''joeuser'' on console 2. Both consoles run different programs in their own user space. In the same vein, different users
can be logged into a Windows system remotely. In this instance, Linux provides capabilities more like a mainframe
than a simple server or workstation.

If you are in a graphical mode, then you can open a ''terminal'' to get access to a console screen. The terminal will
usually have a button on your desktop's task bar, or you can find it under System Tools in the Program menu. You can
also open a terminal from the context menu (right click on the desktop).

'''Commands'''

There are many potential commands available from the console. Some of these tools are only truly useful when writing
scripts. Here are some of the first ones that you'll probably need. Remember that all commands and options are case
sensitive. '''-R''' is different from '''-r''', and will probably do different things. Console commands are almost always
lowercase.

'''cd'''
Moving around in directories uses the familiar cd command. The main trick is to remember that in Linux the
forward-slash (/) is used where you are accustomed to using the back-slash (\). The back-slash is still used, but it
specifies that a command should be continued on the next line. This is sometimes done for readability when typing in a
particularly long command.

'''ls'''
Listing files in a directory can be done with the <code>ls</code> command. There are several switches you can use to alter the look
of the listing:

'''Listing files'''

{| class="wikitable"
|-
| <code>ls -l</code>
| Shows a long listing, including files size, date and time, and attributes
|-
| <code>ls -t</code>
| Sorts files by time
|-
| <code>ls -S</code>
| Sorts files by size
|-
| <code>ls -r</code>
| Combined with one of the sorting switches, reverses the order. <code>ls -lt</code> shows the files with the newest one at the top of the list. <code>ls -lrt</code> shows the files with the newest ones at the bottom.
|-
| <code>ls -h</code>
| Human readable. Uses friendly k, M, and G indicators to show file size rather than listing them in bytes.
|-
| <code>ls -a </code>
| Shows all the files in a directory, even the hidden ones
|}

'''cp'''
Copy files with the <code>cp</code> command. The command works essentially the same as the DOS <code>copy</code> command. Essential
switches:

'''Copying files'''

{| class="wikitable"
|-
| <code>cp -R</code>
| Copies files recursively; required if you are copying an entire directory
|-
| <code>cp -f</code>
| Forces the copy and overwrites existing files without asking
|-
| <code>cp -l</code>
| Links files instead of copying; see below
|}

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:80%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Creating links with the copy command'''<<BR>>The <code>cp</code> command can be used to create a quick set of hard links to a file, or to an entire file structure. Use the <code>-l</code> switch to indicate link copying. All directories will be created as directories, but all files will be set up as hard links.<<BR>><<BR>><code>cp -lR /data/accounting/payroll /data/management/hr</code><<BR>><<BR>>The above command will copy the entire directory structure from /data/accounting/payroll and below to /data/management/hr/payroll. All files in the directory structure will be set up as links.  This can be used to provide different views of the same files within a file system. This is also a helpful security technique, allowing access to files from a different directory with different access controls.
|}

'''mv'''
Move files and rename files with the <code>mv</code> command. It works essentially the same as the DOS <code>move</code> command, except
that it will move entire directory structures as well as files.

'''cat'''
View files with the <code>cat</code> command. This is the equivalent of the DOS <code>type</code> command. It will dump the contents of a
file to another file, to the screen, or to another command. <code>cat</code> is short for concatenate, and can be used to sequence
several files together into a larger file.

'''more'''
View information one page at a time with the <code>more</code> command. It works essentially the same as the DOS <code>more</code>
command.

'''less'''
Use <code>less</code> to view a text file with the ability to scroll up and down through the document and search for text patterns.

'''vi'''
Some might say that <code>vi</code> stands for "virtually impossible." It is a text editor that has a long tradition in the Unix world.
<code>vi</code> is not really intuitive, but it is available in almost any Unix-like environment. There is a built-in tutorial for the
version installed in Linux, and once you get used to it, you can do some truly incredible things in a few keystrokes.
Truly, no editor has managed to replace <code>vi</code> for editing password and configuration files.

'''man'''
View documentation for a command with the <code>man</code> command. Man is short for ''manual''. Documentation tends to be
thorough. To learn more about <code>man</code>, type:

<code>man man</code>

'''info'''
<code>info</code> is like <code>man</code> except it provides hyperlinked text to make browsing documentation easier.

----

'''Which shell?'''

One critical difference between DOS/Windows and Linux is that the command shell is a layer separated from the
operating system. The shell environment affects the features you have, such as editable command lines and scrolling
histories. The shell also determines the syntax required to do functions in scripts. In DOS/Windows, there was only
one option for scripting, the lowly .BAT file. It did a lot, but required a good deal of creativity on the part of the script
writer to do more than basic tasks. In Linux, scripts can contain loops and do more than basic conditional statements,
including many things that you expect from a programming language. If you were good at writing .BAT files, shell
scripts are going to let you shine.

The default shell is a parameter in each user account. The typical default shell in Linux is /bin/bash, though others are
available. The <code>man</code> documentation for each shell is actually very good and goes into detail about shells and how they
work. Rather than try to paraphrase that information here, select a shell from the list below and look at its man page.

'''bash'''
The bash shell is a free version of the Bourne shell, the first Unix shell, and includes many additional features. Bash
has editable command lines, a scrollable command history, and tab completion to help avoid typing long file names.

'''csh'''
The C shell uses a "C-like" syntax and has borrowed many features from the Bourne shell, but uses a different set of
internal shell commands.

'''ksh'''
The Korn shell uses the same syntax as the Bourne shell and has included the user-friendly features of the C shell. <code>ksh</code>
is used in many installation scripts and should probably be installed on the system even if it's not your primary shell.

'''tcsh'''
The TC shell is an enhanced version of the C shell and is 100% compatible with it.

'''zsh'''
The Z shell is an enhanced version of the Korn shell with many features found in the bash shell.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Shell game'''<<BR>>You can change your shell at any time by simply executing it from the console. A script can specify the shell it wants to runby putting a shebang (#!) at the top of the file pointing to the desired shell. When the script is executed, it will run in the correct shell, but leave the user's shell environment alone. Here's an example of a line to get ascript to run in C shell:<<BR>><<BR>> #!/bin/csh
|}
----

'''Link me up, Scotty!'''

One compelling feature in the Linux file system is the file link. A link
is a reference to a file, so that you can let files be seen in multiple
locations of the file system. However, in Linux, a link can be treated as
the original file. A link can be executed, edited, and accessed without
having to do anything unusual. As far as other applications on the
system are concerned, a link is the original file. When you make edits
to a file through the link, you are editing the original. A link is not a
copy. There are two kinds of links: a hard link and a symbolic link.

A ''hard link'' can only reference files in the same file system. It provides a reference to the file's physical index (also
called an ''inode'') in the file's system. Hard links do not break when you move the original file around because they all
point to the file's physical data rather than its location in the file structure. A hard-linked file does not require the user
to have access rights to the original file and does not show the location of the original, so it has some security
advantages. If you delete a file that has been hard linked, the file remains until all references have been deleted as well.

A ''symbolic link'' is a pointer to a file's location in the file system. Symbolic links can span file systems and can even
point to files in a remote file system. A symbolic link shows the location of the original file and requires a user to have
access rights to the original file's location in order to use the link. If the original file is deleted, all of the symbolic links
become broken. They will point to a non-existent location in the file system.

Both types of links can be made with the command <code>ln <source> <target></code>. By default <code>ln</code> will make a hard
link. The <code>-s</code> switch will make a symbolic link.

<code># Create a hard link from MyFile in the current</code>
<code># directory to /YourDir/MyFile</code>
<code>ln MyFile /YourDir</code>

<code># Create a symbolic (soft) link from MyFile in</code>
<code># the current directory to /YourDir/YourFile</code>
<code>ln -s MyFile /YourDir/Yourfile</code>

In the above examples, MyFile, /YourDir/MyFile, and /YourDir/Yourfile are all treated as the same file.

'''Coming out of your shell'''

Learning to work from the console is a necessary skill for Linux administration. There are tools to avoid the console,
but you will always be more limited by what you can do through a tool. Accessing a console is easy, and accessing
command documentation is easy too with the <code>man</code> and <code>info</code> commands.

----

'''Resources'''
* Check out the other parts in the Windows-to-Linux roadmap series (''developerWorks'', November 2003).
* Get started with the vi editor by following the tutorial "vi intro -- the cheat sheet method" (''developerWorks'', ).
* "Technical FAQ for Linux users" (''developerWorks'', July 2001) offers another perspective on making the change from Windows to Linux.
* "What good is a Linux client?" chronicles one man's experience in changing his work environment over from Windows to Linux. The companion article, a "Linux glossary for Windows users" is also useful as a stand-alone reference.
* You can browse many man pages online at the GNU Manuals Online page.
* The From DOS/Windows to Linux HOWTO outlines some good quick-start information for people with a DOS or Windows background.
* AllCommands.com is an unusual site which helps to reference and cross-reference commands from various operating systems.
* The tutorial "LPI certification 101 exam prep, Part 1: Linux fundamentals" covers bash, standard Linux commands, and more.
* Learn more about shell scripting in the article series "Bash by example" (''developerWorks'', ).
* Although written for AIX users, the System User's Guide: Operating System and Devices - Shells reference and book sections including AIX Commands Reference - man Command cover a great deal of information that also applies to Linux.
* For getting started with IBM software on Linux, there's no better resource than the Speed-start your Linux app page. You'll find installation tips and links to resources for DB2, Lotus Domino, WebSphere Application Server, WebSphere Studio, and more. You can also sign up to receive a Linux Software Evaluation Kit, containing trial software and training resources.
* Find more resources for Linux developers in the ''developerWorks'' Linux zone.
 
=== Windows-to-Linux roadmap: Part 3. Introduction to Webmin ===
'''A browser-based administration tool'''

In this part, [the author] shows you how to install and use Webmin, a browser-based
administration tool for Linux and other platforms that provides a graphical interface to many
administrative and operational tasks.

One of the challenges when moving from administering a Windows environment to administering Linux is learning
the new tools at your disposal. As an administrator, you want to learn the details of the operating system so that you
can get the most out of it. But while you are learning, you need to get real work done now.

To accelerate your productivity in Linux, we are going to install a program called Webmin. According to the
Webmin.com (see Resources for a link): "Webmin is a Web-based interface for system administration for Unix. Using
any browser that supports tables and forms (and Java for the File Manager module), you can set up user accounts,
Apache, DNS, file sharing, and so on. Webmin consists of a simple Web server, and a number of CGI programs which
directly update system files like <code>/etc/inetd.conf and /etc/passwd</code>. The Web server and all CGI programs
are written in Perl version 5, and use no non-standard Perl modules."

Webmin runs on virtually all Unix-like platforms including Linux, AIX, HPUX, Solaris, OS X, and others. It provides
a Web front end to many administrative tasks in Linux. It can be run from any graphical browser either locally or
remotely. Webmin can be secured with SSL, to prevent snooping. As you are learning Linux administration, Webmin
is a great time saver. Webmin is also handy to help with the tedious tasks that you have not automated.
Webmin is extensible. The author provides a development guide, and there are several third-party modules available.
You can also design your own modules, so Webmin can always be adapted to fit your needs.

'''Installing Webmin'''

The first step to working with Webmin is to install it. Webmin is included with a few distributions, but it will be just as
easy to download it from the Webmin site (see Resources for a link).

At the time of this writing, the current version is 1.90. The correct method for installation will vary depending in your
distribution. If you are using Red Hat Linux or one of the UnitedLinux distributions (SuSE, Turbo, Connectiva, or
Caldera), then the RPM will be the simplest method of installation. If you are using a different distribution, you will
need to check the documentation for your distribution and the Webmin installation instructions to determine the best
method for you. RPM installation will be assumed.

First check to see if Webmin is installed on your system. From a text terminal, enter the following:

<code>rpm -q webmin</code>

If Webmin is installed on your system, you will receive a version number:

<code>Webmin-1.090-1</code>

or an indication that Webmin is not installed:

<code>package webmin is not installed</code>
{| class="wikitable"
|-
| <tablestyle="float:center; font-size: 0.9em; width:50%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Installing software packages'''<<BR>>Most packages on Linux install just as easily as the Webmin example in this article. For more on installing and deleting packages -- and even on compiling programs from source code -- see Part 9 of this series.
|}
Even if Webmin is already installed, it will probably be a version lower than the current one available for download. In
this case, you can do an upgrade or a fresh install with the following command:

<code>rpm -Uvh webmin-1.090-1.noarch.rpm</code>

A verbose upgrade is done with a progress bar printed with # marks.
{| class="wikitable"
|-
| <tablestyle="float:center; font-size: 0.9em; width:50%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''RPM noarch'''<<BR>>You will notice that Webmin is listed as a "noarch" package. Since RPMs are binary files, they are typically compiled for a particular architecture such as i386 or ppc. Installing the package on the wrong architecture can have bad results. Since Webmin is written in Perl, which is architecture independent, the "noarch" designation is used.
|}
When Webmin is installed, it activates by default. But Webmin does not install with SSL activated. SSL requires the
installation of a Perl module called <code>Net::SSLeay</code>. Until this is installed, Webmin will only be secure to run from the
local console. Securing Webmin will be briefly covered at the end of this article.
{| class="wikitable"
|-
| <tablestyle="float:center; font-size: 0.9em; width:50%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Practical Extraction and Reporting Language (Perl)'''<<BR>>Perl is a multi-platform interpreted programming language that has been around since 1987. It is available on a wide variety of platforms, including Windows, and provides sophisticated scripting capabilities. Perl excels at text-processing and became very popular for Web CGI programming. Perl is extensible by adding modules, which are function libraries, also written in Perl. Most modules, and indeed Perl itself, are available under a generous free license called ''The Artistic License'' (see Resources for a link).
|}

'''Using Webmin'''

Access Webmin through your favorite Web browser. Two of the tools, a file explorer and a telnet/ssh client, are
applet-based and will require a Java Runtime Environment to be installed on your browser. These tools are handy, but
not critical. All of the other modules have no special requirements.

To begin using Webmin, point your browser to port 10000 on the system. With a browser on the local system, you
would use <code>http://localhost.localdomain:10000/</code>. Webmin will first bring you to a login screen.

Webmin users are separate from operating system users. This allows you to set up users for administration through
Webmin that are not in the normal Unix authentication scheme. However, if you have users that you want to be able to
use Webmin, you can enter them into the Webmin user list and have Webmin authenticate them through Unix facilities
rather than through its internal mechanisms. Access to Webmin modules can be controlled for each user. Helpdesk
staff could have access to just password functions, while other staff could have access to all modules, for example.

A root user is automatically created with the system's root password upon installation. Webmin logs activity by login,
so in a multi-admin environment, it would probably be better to create an admin group with the rights of the root user,
and create users for each individual who works on the system. Your first login must be as root.

'''Webmin sections'''
{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''root user'''<<BR>>In Linux, the primary administrative user is called ''root''. The root user has full control over all aspects of the system. The name of root should never be taken in vain.
|}
The first screen you will see is the Webmin Configuration Section.
This is where you configure Webmin users, configure modules, and
view activity logs. The icons at the top switch between the different module sections in Webmin. All of the modules
are configurable, and you can regroup things to suit your preferences.

'''Figure 1. Webmin configuration screen'''

<nowiki>[[File:figure1.png]]</nowiki>

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Webmin is for users, too'''<<BR>>There is an icon to configure the optional Usermin package, which provides a Web-based tool for users to perform functions, such as password changes, system mail management, and other functions. Usermin does not provide access to system configuration functions. Usermin and Webmin are intended to be complimentary products.
|}
The '''System''' section deals with general operating system configuration.
Here, you configure file systems, users, and groups and the general
boot behavior of the system. You can control the services that are
running on the system and whether they start automatically from the
Bootup and Shutdown icon. Configuration of those services, however,
is in the Servers section. The "Software Packages" tool is of particular
interest. It allows easy viewing of packages installed on your system
and interfaces to distribution update repositories and rpmfind.net, a
common RPM repository on the Internet (see Resources for a link).

The '''Servers''' section has configuration for various services that you
may be running on the system. The BIND and DHCP tools are very convenient. Also the Samba tool is simple to use for configuring file and print shares for Windows and other clients.
Sendmail, the SMTP server, is notorious for having a complex configuration file. The Webmin Sendmail tool keeps
you out of trouble there as well.

'''Figure 2. Webmin servers screen'''

<nowiki>[[File:figure2.png]]</nowiki>

The '''Networking''' section provides tools for configuring the network hardware and some of the complex network
controls, such as firewalling. All the tools communicate with the standard configuration files, so anything you do in
Webmin is reflected in the console tools.

The '''Hardware''' section is for configuration of physical devices, mostly printers and storage devices. The Logical
Volume Management (LVM) tool is particularly interesting as it helps you visually manage dynamic volumes on your
Linux system.

The '''Cluster''' section contains tools you would use if you were clustering systems. A cluster, in this context, is a set of
related systems that need to have their configurations synchronized. Systems can synchronize users, groups, packages,
and other things with system failure detection. These tools will help you set up hot failover systems and other systems
where synchronization is important. Clustering is an advanced topic and will probably require installation of packages
not included with your Linux distribution.

The '''Others''' section contains miscellaneous utilities that you may find useful. The "SSH/Telnet Login" and "File
Manager" tools are applet-driven, and cannot be run unless your browser has an active JRE. The "Perl Modules" tool is
very useful for keeping up with Perl modules and will interface directly to CPAN on the Internet. The "File Manager"
tool provides an Explorer-like view of the server's file system, allowing you to move and copy files around without
passing them through your workstation's memory, if you are working remotely. The "SSH/Telnet Login" tool is a
remote shell console that will allow you console access through your browser.

'''Summary'''

Webmin is a browser-based administration application written in Perl. Webmin is extensible and available for other
Unix-like operating systems besides Linux. Once installed, Webmin is accessed through a special port, typically
10000, either locally or from a remote browser. It provides point-and-click interfaces to a variety of Linux
administration tasks, including user management, network firewalling, and network device configuration.

Webmin is free to install and use, and is a good way to manage a working Linux environment while you are making a
transition from the graphical tools of Windows. Webmin tools are a front end to the console-based tools, so
configuration is consistent, and administration can be done from either set of tools safely.

----

'''Resources'''
'''Learn'''
* Read the other installments in this Windows-to-Linux roadmap (developerWorks, November 2003).
* Learn about Perl at Perl.org. The CPAN network is the home of many useful Perl modules and like software.
* Perl is released under the free Artistic License, which allows you to publish or keep private your changes to the source code.
* Learn more about Perl and keep up with developments in the Perl community with the Cultured Perl column on developerWorks.
* For additional information, see these developerWorks articles: "Understanding Linux configuration files", "Using the xinetd program for system administration", "Automating UNIX system administration with Perl", and "Administer Linux on the fly".
* More information on transitioning to Linux awaits you on the developerWorks New to Linux page.
* Find more resources for Linux developers in the developerWorks Linux zone, including our newest how-to tutorials.
* Hone your skills in Linux basics and systems administration with our certification exam study guides. Whether you choose to take the exams or not, our Linux skill-building tutorial series will immerse you in Linux fundamentals as well as advanced topics.
* Learn how to acquire kernel source, configure and boot your new kernel, add a feature, fix a flaw, or just have fun tinkering with operating system source code in our Hacking the Linux kernel tutorial series. Hack and be free.
* IBM developerWorks technical events and Webcasts are a great way to learn more about Linux as well as IBM products that run on Linux.
* The Linux at IBM site offers software, links, end-to-end Linux solutions, and more.
* The Linux Documentation Project is a repository of Linux documentation including documents about individual software, HOWTO documents, FAQs, and more.
* Linux Online! offers non-partisan Linux news and information.
* The O'Reilly Network is an excellent resource for technical books on Linux.

'''Get products and technologies'''
* Download the Webmin tool at its home page, and before using it remotely, read Securing Webmin with SSL.
* RPMFind is an RPM repository where you will find scads of useful (and many less-useful, but fun) programs.
* Order the SEK for Linux, a two-DVD set containing the latest IBM trial software for Linux from DB2®, Lotus®, Rational®, Tivoli®, and WebSphere®.
* Build your next development project on Linux with IBM trial software, available for download directly from developerWorks.

'''Discuss'''
* Participate in the discussion forum.
* Get involved in the developerWorks community by participating in developerWorks blogs.

=== Windows-to-Linux roadmap: Part 4. User administration ===
'''Passwords, groups, and their shadows'''

Administering users in Linux is both very similar to and very different from administering Windows users. Both
systems are multi-user, and control access to resources is based on user identity. Both systems allow collecting users
into groups so that access control can be done more easily without having to touch many users for each change. From
there, the two systems begin to diverge.

'''The super user'''

In Linux, the Super User is called ''root''. The root user can control every process, access every file, and perform any
function on the system. Nothing can ever be hidden from root. Administratively speaking, root is the supreme being. It
is, therefore, very important that the root account be protected by having a secure password. You should not use root
for day-to-day tasks.
Other users can be given root privileges, but this should be done with care. Usually you will configure specific
programs to be run as root by certain users, rather than granting broad root access.

'''Creating new users'''

New users can be created either from the console line, or using a tool such as Webmin.

The command to add a user is <code>useradd</code>. For example, to create a new user from the console:

<code>useradd -c "normal user" -d /home/userid -g users\</code>
<code>-G webadm,helpdesk -s\ /bin/bash userid</code>

This command creates a new user called "userid," the last parameter in the command. A comment is entered that says
"normal user." Userid's home directory will be "/home/userid." Userid's primary group will be users, but userid will
also be placed in the "webadm" and "helpdesk" groups. Userid will use the "/bin/bash" shell as the normal console
environment.

Using Webmin, creating a new user is easy and visual. Log into Webmin with your favorite browser, and go to the
'''System''' section. Select the "Users and Groups" tool, and then click '''Create a new user'''.

'''Figure 1. Webmin's Create User screen'''

<nowiki>[[File:figure3.png]]</nowiki>

Fill in the details for the user, and click '''Create'''. The user will be created.

Adding users with GUI system tools is also covered in "Basic tasks for new Linux developers."

----

'''Changing passwords'''

Changing a user's password can be done from the console by using the <code>passwd</code> command:

<code>passwd userid</code>

Only root can change the password for another user with the <code>passwd</code> command. When the command is entered, you
will be prompted to enter, then confirm, the password you are setting. If they match, then the user tokens are updated
and the password is changed. A user can also change his own password from the console by typing <code>passwd</code>; in this
case, the user is prompted for his old password prior to entering the new one.

Most Linux distributions install with a ''password cracker'' module activated for password changes. This module will test
a password to see if it follows good password practices. If not, a warning will be given that the user is using a bad
password. Depending on your configuration, a user may be required to use a secure password before it will be
accepted. Root may be warned when a password is set, but the action cannot be stopped.

In Webmin, a password is changed using the "Change Passwords" module from the '''System''' section. Select a user from
the list and enter the new password into the blanks.

----

'''Deleting users'''

From the console, users are deleted using the <code>userdel</code> command.

<code>userdel -r userid</code>

The optional '''-r''' switch will delete the user's home directory and all its contents in addition to the user. If the directory
is to be preserved, omit the '''-r''' switch. This switch will not automatically delete all the files on the system that belong to
the user, just the home directory.

----

'''How users are organized'''
Linux configuration is text based. So all users in Linux reside in a file called /etc/passwd. You can view the file one
page at a time with the <code>more</code> command:

<code>more /etc/passwd</code>

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''the /etc directory'''<<BR>>Remember that most configuration files for Linux live in the /etc directory.
|}
The construction of this file is fairly straightforward. Each line contains a new user with parameters separated by a colon.

<code>userid:x:75000:75000::/home/userid:/bin/bash</code>

The first column contains the user name. The second column contains the user's password. The third column contains
the user's numeric id. The fourth column contains the numeric id for the user's primary group. The fifth column
contains the user's full name, or a comment. The sixth column contains the location of the user's home directory.
Normally this directory lives in the /home directory and has the same name as the user id. The seventh column
contains the user's default console shell.

'''Password file structure'''

{| class="wikitable"
|-
| Login ID
| Password
| User ID
| Group ID
| Comment
| Home directory
| Default shell
|-
| userid
| x
| 75000
| 75000
| 
| /home/userid
| /bin/bash
|}
Notice that the example above has an "x" in the Password column. This does not mean that the user has a password of
"x." At one time passwords were normally stored in plain text within this file. This configuration is still possible, but it
is rare because of the implications. The solution was to create something called a ''shadow password''. An "x" is placed
in the password portion of the /etc/passwd file, and an encrypted version of the password goes into the /etc/shadow
file. This technique improved the security by separating the user information from the password data. The MD5
password encryption algorithm further improved security by allowing more robust passwords. An example of a
shadow password entry is below:
{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Shadow passwords and user rights'''One of the idiosyncrasies of Linux user management that is a legacy of the UNIX style is the password file. A user who logs in must be able to read the /etc/password file to see if his username exists. Having the passwords contained in the same file would enable potential crackers to discover passwords; they could download the /etc/passwd file and have the names and scrambled passwords to work on with a separate brute force tool. A shadow password file does not need to be world readable, so crackers would not have the passwords in any form to work with. This approach is still not optimal, because it provides some user information to a potential cracker. A better option is to keep users in a separate repository such as LDAP.
|}

All of the shadow password function is handled behind the scenes, and you will rarely need to do anything more with
it than turn it on.

'''Groups'''

Groups in Linux are much the same as in Windows. You create a group and add members into the group's list. Then
resources can have rights assigned by group. Members of a group have access to a resource associated with that group.

Creating a group is simple, using the console command groupadd:

<code>groupadd mygroup</code>

This will create a group with no members called "newgroup." Groups live in a file called /etc/group. Each group is
listed on a separate line like the following:

<code>mygroup:x:527:</code>

The first column shows the name of the group. The second column is a password. Again, the "x" indicates that the real
password is stored in a shadow file called /etc/gshadow. The third column is a numeric index for the group. Everything
after the third column will be the group members' user ids separated by commas.

To add members to the group, use the <code>gpasswd</code> command with the <code>-a</code> switch and the user id you wish to add:

<code>gpasswd -a userid mygroup</code>

Remove users from a group with the same command, but a <code>-d</code> switch rather than <code>-a</code>:

<code>gpasswd -d userid mygroup</code>

It is also possible to make changes to groups by editing the /etc/group file directly.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Taking care in editing the passwd file'''<<BR>><<BR>>The real danger to editing the /etc/passwd and /etc/group file directly is accidental duplication of an id number. All resources use the id number rather than the name of the user or group. If you accidentally duplicate an id number, then you may grant access to things you did not intend. For example, if you change a user's id number to 0, which is root, when userid logs in, that userid will be root! Also if you delete a user or group line in the file, that user or group is deleted.<<BR>><<BR>>These are errors a human would make. The tools keep that straight. However, sometimes a quick edit to the /etc/group file is the quickest fix to a simple problem. Just bear in mind that you are dealing with some real power when you edit those files.<<BR>>Be careful.
|}
Groups can be created, edited, and destroyed in Webmin with the same
tool used above for working with users.

----

'''User and group associations'''

While this is not the place for a thorough discussion on access control,
you will need some idea about how users and groups are applied to
files. If you look at a long directory listing of a file, you'll see
something like the following.

<code>-rw-r--r-- 1 userid mygroup 703 Jun 23 22:12</code>
<code>myfile</code>

Ignoring the other columns for the moment, look at the third, fourth,
and last columns. The third column contains the name of the owner of
the file, userid. The fourth column contains the group associated with
the file, mygroup. The last column is the file name. Each file can have
only one owner and one group. It is possible to assign rights to Other,
the users who don't fall into either category. Think of Other as the
equivalent of the Windows group Everyone.

A single file owner is common in operating systems, but the single group ownership feels limiting to administrators
new to the technique. It is not. Since users can be members of any number of groups, it is simple to create new groups
to handle resource security. In Linux, group definitions tend to be based more on the resource access required than on
business units. If resources are logically organized on the system, then create more groups to finely tune access to
resources.

More detailed information about associating users and groups is in the Resources section at the end of this article. For
details on how to change file permissions, see <code>man chmod</code>.

----

'''Summary'''

Users and groups work essentially the same way in Linux that they do in Windows, except that only one group can be
associated with a system resource. To think about groups in Linux, consider groups to be cheap and don't be afraid to
create a lot of them for a complex environment. Create your groups based on resource access rather than on business
units.

User and group information are stored in the /etc/passwd and /etc/group files, respectively. Your system will also
probably have an /etc/shadow and /etc/gshadow file, which contain the encrypted passwords for added security. It is
possible to work with users and groups by editing these files directly, but this should be done with great care.

All user and group functions can be handled from the console, which makes them scriptable. Tools, such as Webmin,
also provide graphical ways of working with users and groups.

----

'''Resources'''
'''Learn'''
* Check out the other parts in the Windows-to-Linux roadmap series (developerWorks, November 2003).
* IBM Directory Server implements the Lightweight Directory Access Protocol (LDAP) for accessing directory services, especially those that are X.500 based. Read "Authenticating Linux users with IBM Directory Server" (developerWorks, February 2005) for details.
* File permissions and security are addressed in Chapter 3 of the Introduction to Linux guide at the Linux Documentation Project.
* The University of Maryland shares words of wisdom on the selection of secure passwords.
* The Red Hat Linux Manual documentation offers more detail on the use of Shadow password file.
* The Linux Shadow Password HOWTO gives background history and rationale on the shadowing system as well as step-by-step guidelines for implementation.
* System security is a vast and complex topic, but in an interconnected world, it affects everyone. Luckily, it is never too early nor too late to get started with it. The documents Adding Security to Common Linux Distributions and Strategies for Keeping a Secure Server will help you to do just that.
* "Addressing security issues in Linux" (developerWorks, June 2001) will help you get started with basic security.
* More information on transitioning to Linux awaits you on the developerWorks New to Linux page.
* Find more resources for Linux developers in the developerWorks Linux zone, including our newest how-to tutorials.
* Hone your skills in Linux basics and systems administration with our certification exam study guides. Whether you choose to take the exams or not, our Linux skill-building tutorial series will immerse you in Linux fundamentals as well as advanced topics.
* Learn how to acquire kernel source, configure and boot your new kernel, add a feature, fix a flaw, or just have fun tinkering with operating system source code in our Hacking the Linux kernel tutorial series. Hack and be free.
* IBM developerWorks technical events and Webcasts are a great way to learn more about Linux as well as IBM products that run on Linux.
* The Linux at IBM site offers software, links, end-to-end Linux solutions, and more.
* The Linux Documentation Project is a repository of Linux documentation including documents about individual software, HOWTO documents, FAQs, and more.
* Linux Online! offers non-partisan Linux news and information.
* The O'Reilly Network is an excellent resource for technical books on Linux.

'''Get products and technologies'''
* Order the SEK for Linux, a two-DVD set containing the latest IBM trial software for Linux from DB2®, Lotus®, Rational®, Tivoli®, and WebSphere®.
* Build your next development project on Linux with IBM trial software, available for download directly from developerWorks.

'''Discuss'''
* Participate in the discussion forum.
* Get involved in the developerWorks community by participating in developerWorks blogs.

=== Windows-to-Linux roadmap: Part 5. Linux logging ===
'''Working with logs'''

In this part, we track, manipulate, and rotate logs for security and informational
purposes.

One of the keys to success in managing any system is to know what is happening on the system. Linux offers
exceptional logging, and the detail in the logs is configurable.

Linux logs are in plain text, so you can search and read them without having to use special tools. You can also write
scripts that scan through logs and perform automatic functions based on the contents.

Linux logs are contained in the /var/log directory. There are several log files that are maintained by the system, but
other services and programs may put their log files here too. Most logs are only readable by root, but that can be
changed by simply changing the access rights to the file.

'''/var/log/messages'''

The messages log is the core system log file. It contains the boot messages when the system came up as well as other
status messages as the system runs. Errors with IO, networking, and other general system errors are reported in this
file. Other information, such as when someone becomes root, is listed here as well. If services are running, such as
DHCP servers, you can watch the action in the messages file. /var/log/messages is generally your first place to look
when you are troubleshooting.

'''/var/log/XFree86.0.log'''

This log shows the results of the last execution of the Xfree86 Xwindows server. If you are having problems getting
the graphical mode to come up, this file will usually provide answers as to what is failing.

'''Other logs'''

There will be other log files in the /var/log directory depending on your distribution of Linux and the services and
applications that you are running. For example, there may be logs associated with running a mail server, resource
sharing, automatic tasks, and others.

'''Ready? Rotate!'''

You will see some files in the /var/log directory that end with a number. These are rotated archives. Log files can get
rather large and cumbersome. Linux provides a command to rotate these logs so that you don't have current log
information mixed with older irrelevant data. Generally logrotate runs automatically on a timed basis, but it can
also be run manually. When executed, <code>logrotate</code> will take the current version of the log files and add a ".1" to the end
of the filename. Then any other previously rotated files are sequenced with ".2," ".3," etc. The larger the number after a
filename, the older the log is.

You can configure the automatic behavior for logrotate by editing the /etc/logrotate.conf file. Learn the full details
about <code>logrotate</code> with <code>man logrotate</code>.

----

'''Log tools'''

Any text tool can be used to work with log files. Here are some tools that are particularly helpful.

'''dmesg'''
To get a quick view of the boot log for the last system boot, use the command dmesg. It generally puts out a lot of
text, so you will generally want to pipe it through a viewer.

<code>dmesg | more</code>

The command above will show the boot messages one screen page at a time.

'''tail'''
Sometimes you want to keep an eye on a log file as activity is occurring. Tail is designed to show the last few lines
of a text file. By adding the -f switch, tail will continue to show new output as it occurs.

<code>tail -f /var/log/messages</code>

The command above will show the last ten lines of /var/log/messages, then continue to monitor the file and output any
new activity. To stop the tail -f command, use Ctrl + C to break the processing.

'''more'''
More works the same as the DOS version. You can point it to a file, or pipe output through it to see the information
one screen page at a time. For example, to show the contents of the Xfree86 startup log file one screen page at a time:

<code>more /var/log/XFree86.0.log</code>

Use "q" or [Ctrl]-C to stop looking at a file.

'''less'''
Less is another text viewer, but it allows you to scroll through a file and search for information.

<code>less /var/log/messages</code>

The command above will display the contents of the /var/log/messages file. Use "q" to quit viewing the file. Use "h" to
get help on using <code>less</code>.

'''logger'''
You may want to put your own messages into the log file. You could just append the log message to the correct text
file, but you would have to duplicate the log information style. Also, you would have to change your code if the
logging system had been customized. The logger command lets you send your own messages to the logging facility.
Use it in scripts to provide messages about execution and errors.

----

'''Customized logging'''

There are two services, or daemons, that control logging, <code>klogd</code> and <code>syslogd</code>. <code>klogd</code> only deals with kernel
messages. <code>syslogd</code> deals with other system messages, such as applications. You can configure the behavior of both
by editing the files /etc/syslog.conf and /etc/sysconfig/syslog. Full custom logging is beyond the scope of this article,
but full details can be found in the Resources listed at the end of this article. You can also learn much by looking at the
man page for /etc/sylogd.conf.

Essentially, each message generated by software provides some information to identify where the message came from
and what message it is. The /etc/syslog.conf file allows you to specify what you want done with that kind of message.
You can dump it to the messages file. You can dump it to a custom file. You can have it sent to a remote host where
that host will process it according to its own syslogd configuration. Remote logging is an excellent security feature. By
placing your logs on a remote system, you can prevent a security breach from easily covering its tracks by altering the
log files.

Here is an example of customized logging taken from the <code>man /etc/syslog.conf</code> page:

'''Customized logging'''
{| class="wikitable"
|-
| # Kernel messages are first, stored in the kernel<<BR>># file, critical messages and higher ones also go<<BR>># to another host and to the console<<BR>>#<<BR>>kern.* /var/adm/kernel<<BR>>kern.crit @finlandia<<BR>>kern.crit /dev/console<<BR>>kern.info;kern.!err /var/adm/kernel-info
|}

The first rule directs any message that has the kernel facility to the file /var/adm/kernel.

The second statement directs all kernel messages of the priority crit and higher to the remote host finlandia. This is
useful, because if the host crashes and the disks get irreparable errors, you might not be able to read the stored
messages. If they're on a remote host, too, you still can try to find out the reason for the crash.

The third rule directs these messages to the actual console, so the person who works on the machine will get them, too.

The fourth line tells the syslogd to save all kernel messages that come with priorities from info up to warning in the
file /var/adm/kernel- info. Everything from err and higher is excluded.

The ability to customize logging like this provides a great deal of flexibility and control over the Linux environment.

----

'''Log configuration in Webmin'''
Webmin has a module for working with log files.

'''Figure 1. Webmin system log view'''

<nowiki>[[File:figure4.png]]</nowiki>

All configured log files are shown. Click on a log file to edit its configuration.

'''Figure 2. Webmin log edit screen'''

<nowiki>[[File:figure5.png]]</nowiki>
{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Viewing log files from the console'''<<BR>> Since log files in Linux are written in plain text, they do not require a special tool to interpret them. Any text file viewer can show a Linux log file. A browser, such as Mozilla, can display a log file, and provide search capability. Linux also has console tools to view text files. more, shows you a file one page at a time, just like the MS DOS version. The less command will display the file in a read-only viewer, which provides bi-directional scrolling and search capabilities. Try it now by entering <code>less /var/log/messages</code> at the command line.
|}
Or you can click the '''View''' to see the contents of a log file.

The Webmin module interacts with the /etc/syslog.conf file, so
anything you do in one is reflected in the other.

----

'''Logging in your life'''

Log files in Linux are critical to troubleshooting and maintaining your
system. Linux logging is done to text files, so no proprietary tools are
required to view the files. Text files are also easy to use with custom
scripts and programs.

Logs are rotated to keep them from getting too large and to separate the
current information from much older data. Log rotation is configurable.

Logging is highly configurable, and logs can even be stored on a
separate system for security or backup purposes. You can generate
system log messages out of your own scripts and programs that will be
recognized and processed by the syslogd daemon.

----

'''Resources'''
'''Learn'''
* Check out the other parts in the Windows-to-Linux roadmap series (developerWorks, November 2003).
* The syslog.conf man page contains an excellent description of how to configure logging. To access it, type info syslog.conf.
* The syslogd man page has a good overall description of how syslogd works, including security issues. Type info syslogd.
* The developerWorks tutorial "LPI exam 101 prep: GNU and UNIX commands" (November 2005) covers basic text processing from the command line.
* Logging and working with the system logging daemon is covered in the developerWorks tutorial "LPI exam 201 prep: System maintenance" (September 2005).
* "LPI exam 202 prep: System security" (developerWorks, June 2006) covers Linux security in more detail.
* You'll find more information on .config files in "Understanding Linux configuration files" (developerWorks, December 2001).
* Another great resource for those transitioning from Windows to Linux is the Technical FAQ for Linux users.
* In the developerWorks Linux zone, find more resources for Linux developers.
* Stay current with developerWorks technical events and Webcasts.

'''Get products and technologies'''
* Order the SEK for Linux, a two-DVD set containing the latest IBM trial software for Linux from DB2®, Lotus®, Rational®, Tivoli®, and WebSphere®.
* With IBM trial software, available for download directly from developerWorks, build your next development project on Linux.

'''Discuss'''
* Check out developerWorks blogs and get involved in the developerWorks community.

=== Windows-to-Linux roadmap: Part 6. Working with partitions and file systems ===
'''Using disks and devices in Linux'''

In this part, [the author] explores Linux's hierarchical directory structure, and investigate
mounting and devices.

Working with files and storage devices in Linux is different from Windows. There are files and a hierarchical directory
structure, but beyond that you will need to develop a different way of thinking.

'''Listing 1. Directory structure'''
{| class="wikitable"
|-
| /<<BR>>|-- bin<<BR>>|-- boot<<BR>>|-- dev<<BR>>|-- etc<<BR>>|-- mnt<<BR>>|-- opt<<BR>>|   |-- IBM<<BR>>|   |   |-- WebSphereStudio<<BR>>|   |   -- db2<<BR>>|   |-- IBMHttpServer<<BR>>|-- root<<BR>>|-- sbin<<BR>>|-- tmp<<BR>>|-- usr<<BR>>|   |-- X11R6<<BR>>|   |   |-- bin<<BR>>|   |   |-- include<<BR>>|   |   |-- lib<<BR>>|   |   |-- man<<BR>>|   |   -- share<<BR>>|   |-- bin<<BR>>|   |-- dict<<BR>>|   |-- doc<<BR>>|   |-- etc<<BR>>|   |-- include<<BR>>|   |-- lib<<BR>>|   |-- libexec<<BR>>|   |-- local<<BR>>|   |   |-- OpenOffice<<BR>>|   |   |   |-- sbin<<BR>>
|}

'''No drive letters!'''

There are no drive letters in Linux. This is actually quite useful. If you've worked on a Windows system in a complex
networking environment on a robust machine with several devices, you may have found the alphabet lacking. In
Linux, there is just one file structure. It starts with root (/) and all local file systems, all local devices, and all remote
file systems are represented as subdirectories in this structure.

When Linux first boots, it builds this file structure based on information in the /etc/fstab file. Where Windows assigns
drive letters to hard drive partitions and other storage devices, Linux assigns them directories in the root file structure.
The structure of the hierarchy is completely configurable and can be changed on the fly.

----

'''Mount up!'''

The term for adding a device to the file system is ''mounting''. Linux will automatically mount a / (root) file system.
There may also be a separate /boot file system, containing the core kernel boot files. Linux will also mount some
special file systems. The swap space is not shown as a part of the file system, but is handled by the kernel. However,
other special file systems such as proc, are seen as a normal part of the file system, and its contents can be handled just
like normal files.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''What is /proc?'''<<BR>>The /proc file system is an excellent example of the difference between thinking in Windows and thinking in Linux. /proc contains a virtual representation of various aspects of the running system. There is information about IRQ settings, memory usage, loaded device drivers, network status, and much, much more. There is even a file called /proc/kcore, which is a virtual representation of all of the used system memory. Each of these files can be parsed just like a normal text or binary file. Some files can be written to change the behavior of the running kernel, without rebooting. For example, to turn on IP-forwarding for the first active ethernet device on the system, you can use a file command:<<BR>><<BR>><code>echo 1 ></code><<BR>><code>/proc/sys/net/ipv4/conf/eth0/forwarding</code><<BR>><<BR>>The main benefit of such a system is that you can use simple scripting techniques to do very deep and powerful things to your running system.
|}
Other file systems, such as removable media or remote
file systems, will need to be manually mounted. When
mounting a file system, you will need to know the
correct way to reference it from Linux, and have an
empty directory to use as a ''mount point''. For removable
media, Linux will probably create mount points for you
during installation. In Red Hat Linux, the cdrom device
is set up to mount to the directory /mnt/cdrom. That
means that when you put a CD into the CDROM device,
you enter the command:

<code>mount /mnt/cdrom</code>

The CD is added to the file system and the CDROM
device is locked so that it cannot be accidentally
ejected. To access the contents of the CD, simply use
the directory /mnt/cdrom. When you are finished using
the CD, you can remove it from the file system with the
command:

<code>umount /mnt/cdrom</code>

The /mnt/cdrom directory will empty and the CDROM
device will be unlocked. You can now safely eject the CD. The same behavior would be used with other removable
media, such as a floppy drive (/mnt/floppy).

Running mount with no arguments will show you the currently mounted file systems.
{| class="wikitable"
|-
| <tablestyle="float:center; font-size: 0.9em; width:50%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Why all of this locking behavior?'''<<BR>>Remember that Linux is not only multi-user, but also multi-session. That means that several users may be logged into the system, running processes and using resources all at the same time. This is not the same as logging in to use a file share in Windows. Each user can use the ystem just as though they were sitting at the console. Linux maintains stability by not arbitrarily releasing file systems that are currently in use, and by locking CDs so they cannot be ejected until the file system has been unmounted and no one is using it.
|}

'''The /etc/fstab file'''

The association between a device and its mount point is configured in the /etc/fstab file. It can be human-edited, or
maintained with an administrative tool. Here is a sample of /etc/fstab/:

'''Understanding /etc/fstab'''

/dev/hda5 ext3 defaults 1 1
/dev/hda2 /boot ext3 exec,dev,duid,rw 1 2
/dev/hda6 swap swap defaults 0 0
/dev/scd0 /mnt/cdrom auto ro,noauto,exec 0 0
none /dev/pts devpts id=5,mode=620 0 0
none /proc proc defaults 0 0
none /dev/shm tmpfs defaults 0 0

Each line represents a file system to be mounted. The first column identifies the device to be mounted. The second
column contains the mount point, the location for that device in the file system. The third column identifies the file
system type. The fourth column has options for how that file system should be handled. The last column contains flags
about that file system. The first number is either 1 or 0 and indicates whether the system should be copied with a dump
(an option for system backups). The second number is 0, 1, or 2 and indicates the order in which the file system should
be checked upon boot. 0 is not checked at all. 1 is checked first and should be used for the root (/) file system. Other
file systems should be 2.

In the fstab file listed above, the root file system is on the first IDE hard drive in the fifth partition, the first logical
drive in an extended partition. The /boot file system, where the kernel startup files are located, is on the first IDE hard
drive in the second primary partition. Swap space is located in the first IDE hard drive in the sixth partition, the second
logical drive in an extended partition. Other file systems listed show their device as "none." We'll cover these shortly.
For now let's concentrate on physical disks.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Everything's a file'''<<BR>>In Linux, file systems are represented by a file-like name. All of the files in the /dev directory are special files called nodes that link to physical devices through the device driver. This allows you to do some interesting things. For example, to make an ISO image of a CD, you can use the <code>cp</code> (copy) command:<<BR>><<BR>> <code>cp /dev/cdrecorder MyCD.iso</code><<BR>><<BR>> Rather than copying the file structure of the CD, a binary image is copied. This file-centric approach also allows you to alias device names to something more meaningful. For example, there is usually an alias called /dev/cdrom that points to the physical CDROM device, often /dev/hdc. Once that alias is created, you can always refer to the device as /dev/cdrom, which is much easier to remember. This aliasing technique also allows you to standardize scripts across systems that might have different physical configurations.
|}
The options in the fourth column will vary depending on the file
system type. In the example above, / and /boot are mounted with the
"default" options. That, is they are mounted automatically as read-write
with asynchronous I/O. Only root can mount or unmount the device
but users can execute binaries and use the "sticky bit" (covered later).
The file system will be handled as a block character device. For the
/mnt/cdrom, however, the options are different. It is not automatically
mounted and will mount as a read-only file system. Users will be able
to execute scripts and programs in that file system.

'''Adding file systems'''

You can add file systems into /etc/fstab by adding new lines to the file.
As a practical example, I have a RAID device that contains file
resources for use by the department. This device will only contain data
files and will be kept separate from the operating system, so it will be
easy to move the device to another system in case of a hardware
failure. The RAID has already been configured, and is recognized by
Linux as /dev/sdc, the third SCSI device. A journaled ext3 file system
has been created on the first partition, so we can access it as /dev/sdc1.
I want to have this RAID automatically mount into the file system
when the computer boots.

I add the following line to the /etc/fstab:

<code>/dev/sdc1 /data ext3 defaults 0 0</code>

This will cause the RAID to be mounted at boot just like the / and /boot systems. Now I simply create the directory I
specified as my mount point:

<code>mkdir /data</code>

Once this empty directory is created, we can mount the file system into it:

<code>mount /data</code>

The RAID is now associated to /data. If the system ever reboots, then /data will automatically be mounted.

----

'''Partitions'''

Partitions in Linux work essentially the same as they do in Windows. The <code>fdisk</code> console command is used to create
and manipulate partitions. When you execute <code>fdisk</code>, you must point it toward a device. To see the available devices,
use the command <code>fdisk -l</code>.

'''Listing 2. Using fdisk'''
{| class="wikitable"
|-
| [root@cmw-t30 root]# fdisk -l<<BR>><<BR>>Disk /dev/hda: 240 heads, 63 sectors, 7752 cylinders<<BR>>Units = cylinders of 15120 * 512 bytes<<BR>><<BR>>	Device Boot Start 	End 	Blocks 		Id 	System<<BR>>/dev/hda1 		1 		8 		60448+ 		8e 	Linux LVM<<BR>>/dev/hda2 		9 		15 		52920 		83 	Linux<<BR>>/dev/hda3 	* 	16 		1403 	10493280 	c 	Win95 FAT32 (LBA)<<BR>>/dev/hda4 		1404 	7751 	47990880 	f 	Win95 Ext'd (LBA)<<BR>>/dev/hda5 		1404 	5565 	31464688+ 	83 	Linux<<BR>>/dev/hda6 		5566 	5635 	529168+ 	82 	Linux swap<<BR>>/dev/hda7 		5636 	7751 	15996928+ 	b 	Win95 FAT32
|}

The above list was generated from a laptop, so it shows a rather unorthodox structure for a server. It shows one IDE
hard drive with several partitions. If there were other devices, they would be listed as well. For example, a second IDE
hard drive might be shown as /dev/hdb.

Run <code>fdisk</code> again with a device, and you get a short prompt.

'''Listing 3. fdisk on a device'''
{| class="wikitable"
|-
| [root@cmw-t30 root]# fdisk /dev/hda<<BR>><<BR>>The number of cylinders for this disk is set to 7752.<<BR>>There is nothing wrong with that, but this is larger than 1024,<<BR>>and could in certain setups cause problems with:<<BR>>1) software that runs at boot time (e.g., old versions of LILO)<<BR>>2) booting and partitioning software from other OSs<<BR>>   (e.g., DOS FDISK, OS/2 FDISK)<<BR>>Command (m for help):
|}

Entering "m" will give you a menu of commands. You can display the current partition table with "p." You can create,
delete, and modify the type for existing partitions. "l" will show you the full list of partition types available. Write your
changes to the partition table with "w", and exit the program, or quit without saving with "q." Some changes will take
place immediately. Some types may require a system reboot.

Partition rules under Linux are the same as they are in Windows. You are allowed four primary partitions, any of
which can be extended partitions.

'''File system types'''

Linux can handle any file system type that the kernel knows about. A generous number of them come compiled by
default, and new ones can be added. Here are some interesting ones:
* '''ext2:''' The standard Linux file system
* '''ext3:''' The standard Linux file system with journaling added
* '''vfat:''' Microsoft's Fat32 file system
* '''jfs:''' IBM's journaled file system
* '''reiserfs:''' Another popular journaled file system
{| class="wikitable"
|-
| <tablestyle="float:center; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Journaling saves time and data'''<<BR>>Journaled file systems help protect data from unexpected shutdowns. If a volume is shut down without dismounting, there may be unfinished work and files left in an in-between state. With a typical file system, this requires a full check of the volume, which can take a long time for large volumes. A journaled file system keeps a transaction record of each write to the disk for a period of time, such as five seconds. When the volume is not cleanly unmounted, the file system simply rolls back to the last known good state. A volume that would take twenty minutes to come back up now comes up in seconds!
|}

----

'''Formatting partitions'''

Once created, partitions are formatted with the correct version of the mkfs command. File systems will have their own
version of the <code>mkfs</code>, such as the <code>mkfs.ext2</code>, or the <code>mkfs.ext3</code>. These helper scripts let you create a file system
by simply pointing to the partition. Here are some examples:

'''Listing 4. Using mkfs'''
{| class="wikitable"
|-
| # Create an ext2 file system on the third<<BR>># parition of the first IDE hard drive<<BR>>mkfs.ext2 /dev/hda3<<BR>><<BR>># Create an ext3 file system on the first<<BR>># partition of the 2nd SCSI hard drivemkfs.ext2<<BR>>mkfs.ext3 /dev/sdb1<<BR>><<BR>> # Create a jfs file system in an extended<<BR>># partition on the first IDE hard drive.<<BR>>mkfs.jfs /dev/hda5
|}

There are various advanced parameters to affect how the partition is formatted, but for general purposes, the defaults
are fine. Once the partition has been formatted, it can be mounted into the / file system. A file system must be
unmounted to be reformatted.

----

'''Other file system tools'''

Let's take a look at other useful tools.

'''Console tools'''

There are several tools for looking at the condition of disks and file systems.

'''df'''
<code>df</code> stands for "disk free." It reports the amount of disk space used and available on mounted file systems. Useful
switches:

'''Checking disk space'''
{| class="wikitable"
|-
| df -h
| Human readable; uses friendly k, M, and G indicators to show file size rather than listing them in bytes
|-
| df -l
| Limits the listing to local file systems; by default, remote file systems are also listed
|}

'''du'''
du stands for "disk usage." It reports the amount of disk space used by the specified files and for each subdirectory (of
directory arguments). Useful switches:

'''Checking disk usage'''

{| class="wikitable"
|-
| du -a
| Shows counts for all files, not just directories
|-
| du -h
| Human readable; uses friendly k, M, and G indicators to show file size rather than listing them in bytes
|-
| du -c
| Prints a grand total of all arguments after all arguments have been processed; can be used to find out the total disk usage of a given set of files or directories
|-
| du -s
| Displays only a total for each argument
|}
'''fsck'''
This is the program used to check and repair file systems, equivalent to <code>chkdsk</code> in Windows. It will have different
versions for different file system types, just like <code>mkfs. fsck</code> must be run on unmounted volumes, though it is rarely
needed unless the file system was not cleanly unmounted. <code>man fsck</code> and <code>info fsck</code> provide details, as do several
of the Resources included at the end of this article.

'''Webmin'''

Webmin has several tools to work with file systems and partitions.

'''Figure 1. Webmin partition tool'''

<nowiki>[[File:figure6.png]]</nowiki>

'''Hardware, partitions on local disks'''

Each disk and partition is shown with current usage information. Click on a file system to see details. Unmounted
partitions can have their type edited and file systems formatted.

'''System, disk, and network filesystems'''

Mount and unmount file systems listed in /etc/fstab. Common file system types have a wizard for creating entries.
Unrecognized types can be mounted and unmounted from here, but must be hand-edited in /etc/fstab. Most server file
systems can be handled well from here.

----

'''The whole is the sum of its partitions'''

Though there are many similarities in how Windows and Linux handle partitions and file systems, moving from drive
letters to a completely hierarchical tree will probably take some adjustment. As always, there are robust console tools
to work with these functions and configuration files in the /etc directory. Browser-based front ends like Webmin offer
some helpful tools.

----

'''Resources'''
* Check out the other parts in the Windows-to-Linux roadmap series (developerWorks, November 2003).
* The Linux Partition HOWTO looks more closely at the mechanics of partitioning and gives more detail on the available tools.
* While Linux Administration Made Easy is an older reference, it is still useful, as the general procedures and techniques for Linux have remained consistent.
* The Multi Disk System Tuning HOWTO describes how best to use multiple disks and partitions for a Linux system.
* "Installing and configuring SuSE Linux Enterprise Server (SLES) 8" shows how to install and configure SuSE Linux Enterprise Server (SLES) 8, including steps for graphical configuration using YaST.
* The Linux System Administrator's Guide is an introduction to system administration of a Linux system for novices.
* The IBM developerWorks series Advanced filesystem implementor's guide deals with advanced topics, but also introduces you to the different filesystem options that are available under Linux.
* Learn how to Put virtual filesystems to work in your code with this IBM developerWorks article.
* Formatting a new system? First read these: "Partition planning tips" and "Partitioning in action", both from IBM developerWorks.
* "Dual-booting Linux" from IBM developerWorks explains how you can easily have both Windows and Linux on a single machine.
* The IBM developerWorks article "Maximum swappage" can help you to improve the swap performance on your Linux server.
* The Linux Loader, or LILO, has been superseded! Read all about it in the developerWorks tutorial "Getting to know GRUB".
* "Burning CDs on Linux" is easy when you learn how in this IBM developerWorks guide.
* File permissions and security are addressed in Chapter 3 of the Introduction to Linux guide at the Linux Documentation Project.
* Another great resource for those transitioning from Windows to Linux is the Technical FAQ for Linux users.
* For getting started with IBM software on Linux, there's no better resource than the Speed-start your Linux app page. You'll find installation tips and links to resources for DB2, Lotus Domino, WebSphere Application Server, WebSphere Studio, and more. You can also sign up to receive a Linux Software Evaluation Kit, containing trial software and training resources.
* Find more resources for Linux developers in the developerWorks Linux zone.

=== Windows-to-Linux roadmap: Part 7. Networking ===
'''A quick guide to Linux networking'''

In this part, [the author] explores networking, which is one of the things that Linux does
best.

It is almost inconceivable to run a computer in this age without being connected to a network. E-mail, Web browsing,
and file sharing are all as expected as printing and viewing information on a screen.

Fortunately, Linux was made for the network from the very beginning. In fact, networking is one of the things that
Linux does best. Linux supports the popular networking protocols such as TCP/IP and SMB (NetBIOS). Linux also
has sophisticated tools for monitoring and filtering network traffic. Services such as FTP, Windows file and print
sharing, and Web serving are available. Linux even provides facilities for centralized directory services, Virtual
Private Networking (VPN), and remote procedure calls.

'''Network hardware'''

Linux can work with any network hardware for which it has a driver. Linux drivers are compiled into the kernel, either
monolithically or as loadable modules. Many popular network cards are supported by default in the Linux kernel.
When selecting network hardware, it is always good to use a device listed on the "Hardware Compatibility List" (see
Resources for links). Use the most up-to-date version for your Linux distribution.

Generally, if you are using compatible network hardware, your card will be automatically recognized when you install
the system. You can check the network hardware found on your system by using the <code>ifconfig</code> command. By
default, <code>ifconfig</code> shows you active network devices. You see all network devices by adding the <code>-a</code> switch:

'''Listing 1. Using ifconfig'''
{| class="wikitable"
|-
| refname: ifconfig-a<<BR>><<BR>>[root@cmw-t30 root]# ifconfig -a<<BR>>eth0 Link encap:Ethernet HWaddr 00:09:6B:60:8B:1E<<BR>>inet addr:9.41.209.160 Bcast:9.41.209.255 Mask:255.255.255.0<nowiki>[[BR]]</nowiki>UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1<nowiki>[[BR]]</nowiki>RX packets:47255 errors:0 dropped:0 overruns:0 frame:0<<BR>>TX packets:32949 errors:0 dropped:0 overruns:0 carrier:0<<BR>>collisions:0 txqueuelen:100<<BR>>RX bytes:22140365 (21.1 Mb) TX bytes:13519623 (12.8 Mb)<<BR>>Interrupt:11 Base address:0xf000<<BR>><<BR>>lo Link encap:Local Loopback<<BR>>inet addr:127.0.0.1 Mask:255.0.0.0<nowiki>[[BR]]</nowiki>UP LOOPBACK RUNNING MTU:16436 Metric:1<nowiki>[[BR]]</nowiki>RX packets:1308081 errors:0 dropped:0 overruns:0 frame:0<<BR>>TX packets:1308081 errors:0 dropped:0 overruns:0 carrier:0<<BR>>collisions:0 txqueuelen:0<<BR>>RX bytes:183376967 (174.8 Mb) TX bytes:183376967 (174.8 Mb)
|}

In the listing above, there is only one network card in the system, identified as eth0. The <code>lo</code> adapter is a loopback,
used by Linux to talk to itself. We'll look more at the <code>ifconfig</code> command later.

'''Network device names'''

When they are configured, Linux network devices are given aliases, which consist of a descriptive abbreviation and a
number. The first device of a type is numbered 0, and the others are numbered 1, 2, 3, etc. The following naming
conventions are used. The information is taken from the Linux Network Administrator's Guide (see the Resources
section at the end of this article for links).
 '''* eth0, eth1 ...'''<<BR>>These are the Ethernet card interfaces. They are used for most Ethernet cards, including many of the parallel
port Ethernet cards.
 '''* tr0, tr1 ...'''<<BR>>These are the Token Ring card interfaces. They are used for most Token Ring cards, including non-IBM
manufactured cards.
 '''* s10, s11 ...'''<<BR>>These are the SLIP interfaces. SLIP interfaces are associated with serial lines in the order in which they are
allocated for SLIP.
 '''* ppp0, ppp1 ...'''<<BR>>These are the PPP interfaces. Just like SLIP interfaces, a PPP interface is associated with a serial line once it is
converted to PPP mode.
 '''* plip0. plip1 ...'''<<BR>>These are the PLIP interfaces. PLIP transports IP datagrams over parallel lines. The interfaces are allocated by
the PLIP driver at system boot time and are mapped onto parallel ports. In the 2.0.x kernels, there is a direct
relationship between the device name and the I/O port of the parallel port, but in later kernels, the device names
are allocated sequentially, just as for SLIP and PPP devices.
 '''* ax0, ax1 ...'''<<BR>>These are the AX.25 interfaces. AX.25 is the primary protocol used by amateur radio operators. AX.25
interfaces are allocated and mapped in a similar fashion to SLIP devices.

There are many other types of interfaces available for other network drivers. We've listed only the most common ones.

Since Ethernet is the most common configuration, we will focus on that. For more information about other kinds of
connections, see the Resources at the end of this article.

----

'''Network configuration'''

When you installed your distribution of Linux, the networking was configured. You probably already have an active
eth0 from that initial configuration. This configuration is probably adequate for your use right now, but you may need
to make changes over time. We will cover different configuration items related to IP networking and the files and tools
for working with them.

'''Webmin'''

Webmin offers a good set of network configuration tools under '''Networking''', Network Configuration. You can
configure individual interfaces and adjust their current settings or their saved settings. Also the Routing and Gateways,
DNS Client settings, and local host addresses can be configured. Once all of the configurations have been edited, you
can apply them by clicking '''Apply Configuration'''. Rebooting the system is not necessary.

{| class="wikitable"
|-
| <tablestyle="float:center; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Localhost'''<<BR>>The local host addresses are contained in /etc/hosts. This file is equivalent to the '''C:\winnt\system32\drivers\etc\hosts''' file. Entries show aliases for IP addresses and are used to assign names without having to consult a DNS.<<BR>><<BR>><code>127.0.0.1 localhost.localdomain localhost</code><<BR>><code>10.10.10.10 cmw-t30</code>
|}

'''Distribution tools'''

Each distribution has its own tools for configuring network settings. You should consult your particular distribution's
documentation to see what it uses. Each tool provides essentially the same configuration options as the Webmin tool.
Some of them may provide options specific to the distribution.

'''Figure 1. Red Hat 8.x and 9.x use the redhat-config-network tool'''

<nowiki>[[File:figure7.png]]</nowiki>

'''Figure 2. SuSE and United Linux use the YAST tool'''

<nowiki>[[File:figure8.png]]</nowiki>

Manual configuration is also possible, but it is a very deep subject. Please refer to your distribution documentation and
the Resources at the end of this article for information about manual network configuration.

'''Tools to analyze and monitor'''

Linux comes with many tools to monitor networking tasks.

'''ifconfig'''
We used the <code>ifconfig</code> command above to see the status of the ethernet card. However, <code>ifconfig</code> can configure
devices as well as report on them. Suppose you want to set up a temporary network configuration for testing. You
could edit the configuration through the distribution tool, but you would need to note all of the settings to put it back
when you're done. By using <code>ifconfig</code>, we can configure the card quickly without touching the saved settings:

<code>ipconfig eth0 192.168.13.13 netmask 255.255.255.0 up</code>

The command above will set eth0 to the address 192.168.13.13 with a Class C IP address and make sure that it is up.

<code>ipconfig eth0 down</code>

The command above will shut down the eth0 device. See the <code>info ifconfig</code> page for full details on using <code>ifconfig</code>.

'''ifup/ifdown'''
To activate and deactivate network devices using their saved configurations, use ifup and ifdown, respectively.

<code># Bring up eth0 using the saved configuration</code>
<code>ifup eth0</code>

<code># Shut down eth0</code>
<code>ifdown eth0</code>

'''netstat'''

Use the <code>netstat</code> console command to print network connections, routing tables, interface statistics, masquerade
connections, and multicast memberships. <code>netstat</code> has several command line switches to control its function. Here
are some of the common ones:

'''Printing network status'''

{| class="wikitable"
|-
| <code>netstat -p</code>
| Shows the PID and name of the program to which each socket belongs
|-
| <code>netstat -a</code>
| Shows both listening and non-listening sockets
|-
| <code>netstat -t</code>
| Shows TCP connections
|-
| <code>netstat -u</code>
| Shows UDP connections
|-
| <code>netstat -e</code>
| Displays additional information; use this option twice for maximum detail
|}
Here's an example of <code>netstat -tp</code>:

'''Listing 2. Using netstat'''
{| class="wikitable"
|-
| [root@cmw-t30 root]# netstat -tp<<BR>>Active Internet connections (w/o servers)<<BR>>Proto Recv-Q Send-Q Local Address Foreign Address State<<BR>>PID/Program name<<BR>>tcp 0 0 localhost.localdo:29000 *:* LISTEN<<BR>>2389/attvpnctl<<BR>>tcp 0 0 *:10000 *:* LISTEN<<BR>>5945/perl<<BR>>tcp 0 0 *:x11 *:* LISTEN<<BR>>1120/X<<BR>>tcp 0 0 *:ftp *:* LISTEN<<BR>>724/xinetd<<BR>>tcp 0 0 *:ssh *:* LISTEN<<BR>>710/sshd<<BR>>tcp 0 0 *:ipp *:* LISTEN<<BR>>797/cupsd<<BR>>tcp 0 0 *:505 *:* LISTEN<<BR>>1043/rcd<<BR>>tcp 0 0 localhost.localdoma:ipp localhost.localdo:32772 ESTABLISHED<<BR>>797/cupsd<<BR>>tcp 0 0 sig-9-65-39-140.m:44916 sdoprods2.austin.i:1352 TIME_WAIT<<BR>>-<<BR>>tcp 0 0 10.100.100.101:33020 64.12.29.100:5190 ESTABLISHED<<BR>>1433/gaim<<BR>>tcp 0 0 localhost.localdo:44954 localhost.localdoma:ipp TIME_WAIT<<BR>>-<<BR>>tcp 0 0 localhost.localdo:44955 localhost.localdoma:ipp TIME_WAIT<<BR>>-<<BR>>tcp 0 0 localhost.localdo:44897 localhost.localdoma:ipp TIME_WAIT<<BR>>-<<BR>>tcp 0 0 localhost.localdo:44902 localhost.localdoma:ipp TIME_WAIT<<BR>>-<<BR>>tcp 0 0 localhost.localdo:44903 localhost.localdoma:ipp TIME_WAIT<<BR>>-<<BR>>tcp 0 0 localhost.localdo:44900 localhost.localdoma:ipp TIME_WAIT<<BR>>-<<BR>>tcp 0 0 localhost.localdo:44901 localhost.localdoma:ipp TIME_WAIT<<BR>>-<<BR>>tcp 0 0 10.100.100.101:44888 cs9336-61.austin.r:pop3 TIME_WAIT<<BR>>-<<BR>>tcp 0 0 localhost.localdo:32772 localhost.localdoma:ipp ESTABLISHED<<BR>>1246/gnome-cups-man<<BR>>tcp 1 0 localhost.localdo:32774 localhost.localdoma:ipp CLOSE_WAIT<<BR>>1246/gnome-cups-man<<BR>>tcp 0 0 10.100.100.101:33019 cs46.msg.sc5.yahoo:5050 ESTABLISHED<<BR>>1433/gaim<<BR>>tcp 0 0 sig-9-65-39-140.m:35061 d03nm119.boulder.i:1352 CLOSE_WAIT<<BR>>1720/wineserver<<BR>>tcp 0 0 10.100.100.101:33021 64.12.30.4:5190 ESTABLISHED<<BR>>1433/gaim
|}

I use <code>netstat</code> most often to view connections that are in the LISTEN or ESTABLISHED states. LISTEN are the
services on your system that are accepting connections from other machines. ESTABLISHED are the active
connections between your machine and others. Make sure you know all of the LISTEN programs that are running. If
you see something you don't recognize, it could be a security concern. <code>netstat</code> has many options. Type <code>info netstat</code> at the command line for details.

'''route'''
The <code>route</code> console command lets you show and manipulate the IP routing table.

'''Listing 3. Using route'''
{| class="wikitable"
|-
| [root@cmw-t30 plugins]# route|grep -v ipsec<<BR>>Kernel IP routing table<<BR>>Destination Gateway Genmask Flags Metric Ref Use Iface<<BR>>204.146.24.42 10.100.100.1 255.255.255.255 UGH 0 0 0 eth1<<BR>>10.100.100.0 * 255.255.255.0 U 0 0 0 eth1<<BR>>127.0.0.0 * 255.0.0.0 U 0 0 0 lo<<BR>>default 10.100.100.1 0.0.0.0 UG 0 0 0 eth1
|}

Running <code>route</code> with no switches will show the current routing table. You can make very elaborate changes to the
routing table using <code>route</code>.

<code>route add default gw 10.10.10.1</code>

The above command adds a default route (which will be used if no other route matches). All packets using this route
will be gatewayed through "10.10.10.1". The device that will actually be used for that route depends on how we can
reach "10.10.10.1" -- the static route to "10.10.10.1" will have to be set up before.

<code>route add -net 192.56.76.0 netmask 255.255.255.0 dev eth0</code>

The above command adds a route to the network 192.56.76.x via "eth0." The Class C netmask modifier is not really
necessary here because 192.* is a Class C IP address. The word "dev" can be omitted here.

Routing is a very deep subject. Full information about the <code>route</code> options is available with <code>info route</code>.

----

'''Summary'''

Linux was designed for networking from the start. It has built into it sophisticated functions that were previously found
only on high-end enterprise offerings. However, even with all of this power, configuration of Linux networking is no
more complex than configuration in Windows. Tools such as Webmin, redhat-config-network, and YAST allow
graphical configuration. Tools such as <code>ifconfig</code> and <code>route</code> allow viewing and modification of network parameters
from the console or scripts. Tools such as <code>netstat</code> allow viewing of individual network connections and show their
relationships to running processes.

----

'''Resources'''
* Check out the other parts in the Windows-to-Linux roadmap series (developerWorks, November 2003).
* The online Linux Network Administrator's Guide, Second Edition is a single reference for network administration in a Linux environment. Beginners and experienced users alike will find the information on nearly all important administration activities required to manage a Linux network configuration.
* The Linux Ethernet HOWTO has information about which ethernet devices can be used for Linux, and how to set them up (with a focus on the hardware and low-level driver aspect of the ethernet cards).
* The Linux Documentation Project also has a list of HOWTOs by category to help you find relevant documentation easily.
* Hardware Control Lists include the Red Hat Hardware search page, the SuSE Linux Component DataBase, and the UnitedLinux Certified and Compatible Hardware.
* System security is a vast and complex topic, but in an interconnected world, it affects everyone. Luckily, it is never too early nor too late to get started with it. The documents Adding Security to Common Linux Distributions and Strategies for Keeping a Secure Server (which is the twelfth chapter from the earlier referenced Linux Administration Made Easy guide) will help you to do just that.
* The IBM developerWorks article "Linux hardware stability guide" shows you how to diagnose and fix many potential hardware troubles.
* Learn more about networking in the IBM developerWorks articles "Sharing computers on a Linux (or heterogeneous) network" and "Setting up a Local Area Network".
* Emulate Cisco behavior by following the IBM developerWorks article "Build a network router on Linux".
* Employ better security -- the IBM developerWorks article "Connect securely with ssh" shows you how.
* Another great resource for those transitioning from Windows to Linux is the Technical FAQ for Linux users.
* For getting started with IBM software on Linux, there's no better resource than the Speed-start your Linux app page. You'll find installation tips and links to resources for DB2, Lotus Domino, WebSphere Application Server, WebSphere Studio, and more. You can also sign up to receive a Linux Software Evaluation Kit, containing trial software and training resources.
* Find more resources for Linux developers in the developerWorks Linux zone.

=== Windows-to-Linux roadmap: Part 8. Backup and recovery ===

Linux is a stable and reliable environment. But any computing system can have unforeseen events, such as hardware
failures. Having a reliable backup of critical configuration information and data is part of any responsible
administration plan. There is a wide variety of approaches to doing backups in Linux. Techniques range from very
simple script-driven methods to elaborate commercial software. Backups can be done to remote network devices, tape
drives, and other removable media. Backups can be file-based or drive-image based. There are many options available
and you can mix and match your techniques to design the perfect backup plan for your circumstances.

'''What's your strategy?'''

There are many different approaches to backing up a system. For some perspectives on this, you may want to read the
article "Introduction to Backing Up and Restoring Data" listed in the Resources section at the end of this article.

What you back up depends a lot on your reason for backing up. Are you trying to recover from critical failures, such as
hard drive problems? Are you archiving so that old files can be recovered if needed? Do you plan to start with a cold
system and restore, or a preloaded standby system?

----

'''What to back up?'''

The file-based nature of Linux is a great advantage when backing up and restoring the system. In a Windows system,
the registry is very system specific. Configurations and software installations are not simply a matter of dropping files
on a system. Therefore, restoring a system requires software that can deal with these idiosyncrasies. In Linux, the story
is different. Configuration files are text based and, except for when they deal directly with hardware, are largely
system independent. The modern approach to hardware drivers is to have them available as modules that are
dynamically loaded, so kernels are becoming more system independent. Rather than a backup having to deal with the
intricacies of how the operating system is installed on your system and hardware, Linux backups are about packaging
and unpackaging files.

In general, there are some directories that you want to back up:
* /etc<<BR>>contains all of your core configuration files. This includes your network configuration, system name, firewall rules, users, groups, and other global system items.
* /var<<BR>>contains information used by your systems daemons (services) including DNS configurations, DHCP leases, mail spool files, HTTP server files, db2 instance configuration, and others.
* /home<<BR>>contains the default user home directories for all of your users. This includes their personal settings,downloaded files, and other information your users don't want to lose.
* /root<<BR>>is the home directory for the root user.
* /opt<<BR>>is where a lot of non-system software will be installed. IBM software goes in here. OpenOffice, JDKs, and other software is also installed here by default.

There are directories that you should consider not backing up.

* /proc<<BR>>should never be backed up. It is not a real-file system, but rather a virtualized view of the running kernel and environment. It includes files such as /proc/kcore, which is a virtual view of the entire running memory. Backing these up only wastes resources.
* /dev<<BR>>contains the file representations of your hardware devices. If you are planning to restore to a blank system, then you can back up /dev. However, if you are planning to restore to an installed Linux base, then backing up /dev will not be necessary. 

The other directories contain system files and installed packages. In a server environment, much of this information is
not customized. Most customization occurs in the /etc and /home directories. But for completeness, you may wish to
back them up.

In a production environment where I wanted to be assured that no data would be lost, I would back up the entire
system, except for the /proc directory. If I were mostly worried about users and configuration, I would back up only
the /etc, /var, /home, and /root directories.

----

'''Backup tools'''

As mentioned before, Linux backups are largely about packaging and unpackaging files. This allows you to use
existing system utilities and scripting to perform your backups rather than having to purchase a commercial software
package. In many cases, this type of backup will be adequate, and it provides a great deal of control for the
administrator. The backup script can be automated using the cron command, which controls scheduled events in
Linux.

'''tar'''

<code>tar</code> is a classic UNIX command that has been ported into Linux. <code>tar</code> is short for '''t'''ape '''a'''rchive, and was originally
designed for packaging files onto tape. You have probably already encountered tar files if you have downloaded any
source code for Linux. It is a file-based command that essentially serially stacks the files end to end.

Entire directory trees can be packaged with <code>tar</code>, which makes it especially suited to backups. Archives can be
restored in their entirety, or files and directories can be expanded individually. Backups can go to file-based devices or
tape devices. Files can be redirected upon restoration to replace to a different directory (or system) from where they
were originally saved. tar is file system-independent. It can be used on ext2, ext3, jfs, Reiser, and other file systems.

Using <code>tar</code> is very much like using a file utility, such as PKZip. You point it toward a destination, which is a file or a
device, and then name the files that you want to package. You can compress archives on the fly with standard
compression types, or specify an external compression program of your choice. To compress or uncompress files
through bzip2, use <code>tar -z</code>.

To back up the entire file system using tar to a SCSI tape drive, excluding the /proc directory:

<code>tar -cpf /dev/st0 / --exclude=/proc</code>

In the above example, the -c switch indicates that the archive is being created. The -p switch indicates that we want
to preserve the file permissions, critical for a good backup. The -f switch points to the filename for the archive. In this
case, we are using the raw tape device, /dev/st0. The / indicates what we want to back up. Since we wanted the entire
file system, we specified the root. tar automatically recurses when pointed to a directory (ending in a /). Finally, we
exclude the /proc directory, since it doesn't contain anything we need to save. If the backup will not fit on a single tape,
we will add the -M switch (not shown), for multi-volume.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Just in case'''<<BR>> Don't forget that Linux is case sensitive. The <code>tar</code> command should always be executed in lowercase, for example. Switches can be upper, lower, or mixed case. For example <code>-t</code> and <code>-T</code> perform different functions. File or directory names may be mixed case and, like commands and switches, are case sensitive.
|}
To restore a file or files, the <code>tar</code> command is used with the extract
switch (-x):

<code>tar -xpf /dev/st0 -C /</code>

The <code>-f</code> switch again points to our file, and <code>-p</code> indicates that we want
to restore archived permissions. The <code>-x</code> switch indicates an extraction
of the archive. The <code>-C</code> / indicates that we want the restore to occur
from /. <code>tar</code> normally restores to the directory from which the
command is run. The <code>-C</code> switch makes our current directory irrelevant.

The two other <code>tar</code> commands that you will probably use often are the
<code>-t</code> and <code>-d</code> switches. The <code>-t</code> switch lists the contents of an archive. The <code>-d</code> switch compares the contents of the
archive to current files on a system.

For ease of operation and editing, you can put the files and directories that you want to archive in a text file, which you
reference with the <code>-T</code> switch. These can be combined with other directories listed on the command line. The following
line backs up all the files and directories listed in MyFiles, the /root directory, and all of the iso files in the /tmp
directory:

<code>tar -cpf /dev/st0 -T MyFiles /root /tmp/*.iso</code>

The file list is simply a text file with the list of files or directories. Here's an example:

<code>/etc</code>
<code>/var</code>
<code>/home</code>
<code>/usr/local</code>
<code>/opt</code>

Please note that <code>the tar -T</code> (or <code>files-from</code>) command cannot accept wildcards. Files must be listed explicitly.
The example above shows one way to reference files separately. You could also execute a script to search the system
and then build a list. Here is an example of such a script:

<code>#!/bin/sh</code>
<code>cat MyFiles > TempList</code>
<code>find /usr/share -iname *.png >> TempList</code>
<code>find /tmp -iname *.iso >> TempList</code>
<code>tar -cpzMf /dev/st0 -T TempList</code>

The above script first copies all of our existing file list from MyFiles to TempList. Then it executes a couple of <code>find</code>
commands to search the file system for files that match a pattern and to append them to the TempList. The first search
is for all files in the /usr/share directory tree that end in <code>.png</code>. The second search is for all files in the /tmp directory
tree that end in <code>.iso.</code> Once the list is built, then <code>tar</code> is run to create a new archive on the file device /dev/st0 (the first
SCSI tape drive), which is compressed using the gzip format and retains all of the file permissions. The archive will
span Multiple volumes. The file names to be archived will be Taken from the file TempList.

Scripting can also be used to perform much more elaborate actions such as incremental backups. An excellent script is
listed by Gerhard Mourani in his book ''Securing and Optimizing Linux'', which you will find listed in the Resources
section at the end of this article.

Scripts can also be written to restore files, though restoration is often done manually. As mentioned above, the <code>-x</code>
switch for extract replaces the <code>-c</code> switch. Entire archives can be restored, or individual files or directories can be
specified. Wildcards are okay to reference files in the archive. You can also use switches to dump and restore.

----

'''dump and restore'''

<code>dump</code> can perform functions similar to <code>tar</code>. However, <code>dump</code> tends to look at file systems rather than individual files.
Quoting from the dump man file: "dump examines files on an ext2 filesystem and determines which files need to be
backed up. These files are copied to the given disk, tape, or other storage medium for safe keeping.... A dump that is
larger than the output medium is broken into multiple volumes. On most media, the size is determined by writing until
an end-of-media indication is returned."

The companion program to dump is <code>restore</code>, which is used to restore files from a dump image.

The <code>restore</code> command performs the inverse function of dump. A full backup of a file system may be restored and
subsequent incremental backups layered on top of it. Single files and directory subtrees may be restored from full or
partial backups.

Both <code>dump</code> and <code>restore</code> can be run across the network, so you can back up or restore from remote devices. <code>dump</code>
and <code>restore</code> work with tape drives and file devices providing a wide range of options. However, both are limited to
the ext2 and ext3 file systems. If you are working with JFS, Reiser, or other file systems, you will need to use a
different utility, such as <code>tar</code>.

----

'''Backing up with dump'''

Running a backup with dump is fairly straightforward. The following command does a full backup of Linux with all
ext2 and ext3 file systems to a SCSI tape device:

<code>dump 0f /dev/nst0 /boot</code>
<code>dump 0f /dev/nst0 /</code>

In this example, our system has two file systems. One for /boot and another for / -- a common configuration. They
must be referenced individually when a backup is executed. The /dev/nst0 refers to the first SCSI tape, but in a
non-rewind mode. This ensures that the volumes are put back-to-back on the tape.

An interesting feature of <code>dump</code> is its built-in incremental backup functionality. In the example above, the 0 indicates a
level 0, or base-level, backup. This is the full system backup that you would do periodically to capture the entire
system. On subsequent backups you can use other numbers (1-9) in place of the 0 to change the level of the backup. A
level 1 backup would save all of the files that had changed since the level 0 backup was done. Level 2 would backup
everything that had changed from level 1 and so on. The same function can be done with <code>tar</code>, using scripting, but it
requires the script creator to have a mechanism to determine when the last backup was done. <code>dump</code> has its own
mechanism, writing an update file (/etc/dumpupdates) when it performs a backup. The update file is reset whenever a
level 0 backup is run. Subsequent levels leave their mark until another level 0 is done. If you are doing a tape-based
backup, <code>dump</code> will automatically track multiple volumes.
{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Skipping files'''<<BR>>It is possible to mark files and directories to be skipped by <code>dump</code>. The command for this is <code>chattr</code>, which changes the extended attributes on ext2 and ext3 file systems.<<BR>><<BR>><code>chattr +d <filename></code><<BR>><<BR>>The above command adds a flag to a file to tell <code>dump</code> to skip it when doing a backup.
|}

'''Restoring with restore'''

To restore information saved with <code>dump</code>, the <code>restore</code> command is
used. Like <code>tar, dump</code> has the ability to list (<code>-t</code>) and compare archives
to current files (<code>-C</code>). Where you must be careful with dump is in
restoring data. There are two very different approaches, and you must
use the correct one to have predictable results.

'''Rebuild (-r)'''

Remember that dump is designed with file systems in mind more than individual files. Therefore, there are two
different styles of restoring files. To rebuild a file system, use the <code>-r</code> switch. Rebuild is designed to work on an empty
file system and restore it back to the saved state. Before running rebuild, you should have created, formatted, and
mounted the file system. You should not run rebuild on a file system that contains files.

Here is an example of doing a full rebuild from the dump that we executed above.

<code>restore -rf /dev/nst0</code>

The above command needs to be run for each file system being restored.

This process could be repeated to add the incremental backups if required.

'''Extract (-x)'''

If you need to work with individual files, rather than full file systems, you must use the -x switch to extract them. For
example, to extract only the /etc directory from our tape backup, use the following command:

<code>restore -xf /dev/nst0 /etc</code>

'''Interactive restore (-i)'''

One more feature that <code>restore</code> provides is an interactive mode. Using the command:

<code>restore -if /dev/nst0</code>

will place you in an interactive shell, showing the items contained in the archive. Typing "help" will give you a list of
commands. You can then browse and select the items you wish to be extracted. Bear in mind that any files that you
extract will go into your current directory.

----

'''dump vs. tar'''

Both <code>dump</code> and <code>tar</code> have their followings. Both have advantages and disadvantages. If you are running anything but
an ext2 or ext3 file system, then <code>dump</code> is not available to you. However, if this is not the case, <code>dump</code> can be run with a
minimum of scripting, and has interactive modes available to assist with restoration.

I tend to use <code>tar</code>, because I am fond of scripting for that extra level of control. There are also multi-platform tools for
working with .tar files.

----

'''Other tools''']

Virtually any program that can copy files can be used to perform some sort of backup in Linux. There are references to
people using <code>cpio</code> and <code>dd</code> for backups. <code>cpio</code> is another packaging utility along the lines of <code>tar</code>. It is much less
common. <code>dd</code> is a file system copy utility that makes binary copies of file systems. <code>dd</code> might be used to make an image
of a hard drive, similar to using a product like Symantec's Ghost. However, <code>dd</code> is not file based, so you can only
restore data to an identical hard drive partition.

----

'''Commercial backup products'''

There are several commercial backup products available for Linux. Commercial products generally provide a
convenient interface and reporting system, whereas with tools such as <code>dump</code> and <code>tar</code>, you have to roll your own. The
commercial offerings are broad and offer a range of features. The biggest benefit you will gain from using a
commercial package is a pre-built strategy for handling backups that you can just put to work. Commercial developers
have already made many of the mistakes that you are about to, and the cost of their wisdom is cheap compared to the
loss of your precious data.

'''Tivoli Storage Manager'''

Probably the best commercial backup and storage management utility available now for Linux is the Tivoli Storage
Manager. Tivoli Storage Manager Server runs on several platforms, including Linux, and the client runs on many more
platforms.

Essentially a Storage Manager Server is configured with the devices appropriate to back up the environment. Any
system that is to participate in the backups loads a client that communicates with the server. Backups can be scheduled,
performed manually from the Tivoli Storage Manager client interface, or performed remotely using a Web-based
interface.

The policy-based nature of TSM means that central rules can be defined for backup behavior without having to
constantly adjust a file list. Additionally, IBM Tivoli Storage Resource Manager can identify, evaluate, control, and
predict the utilization of enterprise storage assets, and can detect potential problems and automatically apply
self-healing adjustments. See the Tivoli Web site (see the link in the Resources section) for more details.

'''Figure 1. Tivoli Storage Manager menu'''

<nowiki>[[File:figure9.png]]</nowiki>

Backups and restores are then handled through the remote device.

'''Figure 2. Tivoli Storage Manager interface'''

<nowiki>[[File:figure10.png]]</nowiki>

----

'''Go forth and back up'''

The first step to having a good backup is to have a plan. Know the data that you need to preserve and what your
recovery strategy needs to be. Then use the tools that best meet that strategy.

Linux comes with some useful backup tools right out of the box. The two most common are <code>tar</code> and
<code>dump/restore</code>. Both are capable of doing full system backups. Using creative scripting, you can design a custom
backup scheme to back up systems both locally and remotely.

However, writing your own backup scripts can be a large responsibility, especially when it is a complicated enterprise.
Commercial software, such as the Tivoli Storage Manager, cuts across the learning curve and lets you take immediate
control of your backups, but you may have to adjust your strategy to fit what the tools can do.

----

'''Resources'''
* Check out the other parts in the Windows-to-Linux roadmap series (developerWorks, November 2003).
* The Linux Administrator's Security Guide is a security guide with an excellent section on the practicalities of Linux backup and recovery.
* Introduction to Backing Up and Restoring Data is an overview that is independent of operating system or system architecture. In this article, the author explores backup techniques as well as planning backups.
* Linux Administration Made Easy is an older reference, but still useful, as the general procedures and techniques for Linux have remained consistent.
* The Linux System Administrator's Guide is an introduction to system administration of a Linux system for novices.
* The TAO of Backup is an interesting presentation of backup philosophy, presented as philosophy. It is associated with a commercial product, but the information is well written.
* The IBM developerWorks tutorial "Backing up your Linux machines" walks you through the process of creating and following through with a backup strategy.
* Storing backup data and other bits on a CD is easy: learn how in the IBM developerWorks article "Burning CDs on Linux".
* If you're transitioning to Linux from a Windows environment, you'll also want to read the Technical FAQ for Linux users.
* The Tivoli Storage Manager was voted Best Data Storage Solution at LinuxWorld 2003. Learn more about Tivoli Storage Manager for Linux from the Linux at IBM site.
* The Tivoli product page has more information on Tivoli including security and privacy features.
* File permissions and security are addressed in Chapter 3 of the Introduction to Linux guide at the Linux Documentation Project.
* Find more resources for Linux developers in the developerWorks Linux zone.

=== Windows-to-Linux roadmap: Part 9. Installing software ===
'''Using pre-compiled RPMs and compiling applications from source'''

In this final part, we download and compile a software package, discuss the pros
and cons of automated package management, and get to know the RPM system.

One of the first things you notice when you install Linux is that there are so many packages available with your
distribution. Most distributions come with the Linux operating system, installation tools, and administration tools.
Then they include Internet tools, development tools, office tools, games, and some things that you haven't even heard
of. It is not uncommon for a Linux distribution to come with ''thousands'' of available packages. If you didn't select
"install everything," then some subset of these packages were installed.

Now you may be wondering "How do I remove packages I don't want? How do I install things I missed? Can I use
software that didn't come with my distribution?"

'''RPMs'''

As Linux installed, you probably noticed a lot of information about RPMs being installed. ''RPM'' stands for Redhat
Package Manager, a contribution by Redhat that has become a standard for managing software on Redhat and
UnitedLinux as well as on many other distributions.

Essentially, an RPM is a package, containing software for Linux ready to install and run on a particular machine
architecture. For example, we installed the webmin package from an RPM in "Part 3. Introduction to Webmin." All of
the software initially loaded in your distribution was installed from an RPM.

'''Anatomy of an RPM'''

An RPM is a package of files. It includes a .spec file, which provides information about the package, its function, and
its dependencies (what packages must be in place before it can run). The .spec also contains a manifest of files in the
package, where they must be loaded on the system, and what their initial permissions will be. The RPM also contains a
pre-install script, which is written by the package developer. Then the RPM contains the compiled binary files. Finally,
the RPM contains a post-install script.

RPM layout

{| class="wikitable"
|-
| .spec
| pre-install script
| binary file
| binary file
| ...
| binary file
| post-install script
|}
When an RPM is installed, the system first looks to see if the dependencies for the package are satisfied. If not, then
the installation terminates unless you specify options to force an install anyway.

If all is clear, the pre-install script runs. This script can do anything. Normally it creates users and directories.
However, it can do many types of dynamic configuration, even custom-compile source code for the running system.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''Know where your RPMs have been'''<<BR>>When RPMs install, they copy files onto your system and execute scripts. Since RPM is run as root, all of these functions are performed as root. It is therefore important that you know the origin of an RPM before you install it on your system. Just as with Windows software, malicious code can be contained inside an RPM as easily as any other package. RPMs from the manufacturer are generally safe, but be cautious about randomly downloading and installing things from unknown sources.
|}
If the pre-install script completes successfully, then the binary files are
copied onto the system according to the manifest. Once all of the files
have been copied and their permissions are set, then the post-install
script is run. Again, this script can do almost anything.

Once all of that is completed, the information about the package is
added to the RPM database, and the installation is complete. With this
simple system, it is possible to perform all of the functions that could
be done with a more elaborate commercial installer.

'''The RPM database'''

The piece of the RPM that adds elegance is the RPM database. This
database typically lives in the /var/lib/rpm directory and holds
information about every RPM installed on the system. The database
knows the dependency relationships between packages and will warn if
removing a package could cause other packages to break. The database knows about every file that was originally
installed with a package and its original state on the system. It also knows the locations of the documentation and
configuration files for each package. This may sound like a lot of information, and it is. But it isn't bloated and bulky.
On a system containing 1,066 packages, comprised of 203,272 files, the database files are only 45 MB! RPM uses the
database to check dependencies when packages are loaded and unloaded. Users can also query the database for
information on packages.

----

'''Using RPM'''

The program to work with RPM packages is appropriately named <code>rpm. rpm</code> runs in several different modes, but the
most common tasks are install, upgrade, query, verify, and erase.

'''rpm -i (install)'''

When you install a package for the first time, you will use the <code>-i</code> or install mode. Simply point the rpm to a binary
package and execute it. The rpm will be installed on your system. Installation normally takes seconds. Often when
installing a package, I will add the <code>-v</code> (verbose) switch to provide more information about the process, and the <code>-h</code>
(hash bar) switch to provide progress updates via hash (#) marks printed on the console as the package is installed.
Here's an example of installing a package:

'''Listing 1. Installing MyPackage'''
{| class="wikitable"
|-
| $ rpm -ivh MyPackage-1.0.0.i386.rpm<<BR>>Preparing... ########################################### [100%]<<BR>>1:MyPackage ########################################### [100%]
|}

That's it! MyPackage is now installed and ready to use.

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''rpm must be run as root'''<<BR>><code>rpm</code> installs and erases must be done as root, because access is required to the file system and the rpm databases.
|}
'''rpm -e (erase)'''

To remove an installed package, use the <code>-e</code> switch to erase it. <code>rpm</code> will
use the database to remove all of the files for the package. If there are
other packages installed that depend on the one you are removing, <code>rpm</code>
will abort. You will have to force the erase with the <code>nodeps</code> switch.
(<code>nodeps</code> can also be used to force an installation.) Be ''very'' careful when using this switch to force an install or erase.
Removing packages that others are dependent on can have unfortunate results. Here is the command to remove the
package we installed above:

<code>$ rpm -e MyPackage</code>

Notice that the full version of the package is not necessary to remove it. The full name was required at installation
because we were pointing to a file name. Installed packages are referenced by their name only. The package's name is
everything up to the version number.

'''rpm -V (verify)'''

The verify switch is very useful. It compares the current state of a package's files to their original state upon
installation. Differences are shown using a code:

'''Results of verifying files'''

{| class="wikitable"
|-
| S
| File Size differs
|-
| M
| Mode differs (includes permissions and file type)
|-
| 5
| MD5 sum differs
|-
| D
| Device major/minor number mis-match
|-
| L
| readLink(2) path mis-match
|-
| U
| User ownership differs
|-
| G
| Group ownership differs
|-
| T
| mTime differs
|}

If you were to run <code>rpm -V</code> on a package and discover that the size had changed for an executable, that would be a
possible sign of a security breach.

'''rpm -U (upgrade)'''

Once a package has been installed, any attempt to install a package with the same name will result in a message that
the package is already installed. If you want to update a package to a later version, use the <code>-U</code> switch to upgrade.
Upgrade has another affect. When upgrade is run on multiple package names, it will try to put the packages in order of
dependencies. In other words, required packages will be installed first. The upgrade switch can be used whether or not
a package is already installed, so many people use it for installs as well as upgrades instead of using the <code>-i</code> switch.
Here is an example of using the upgrade switch to load several rpm packages:

'''Listing 2. Interactive upgrade'''
{| class="wikitable"
|-
| $ rpm -Uvh My*.rpm<<BR>>Preparing... ########################################### [100%]<<BR>>1:bMyPackageDep ########################################### [ 50%]<<BR>>1:aMyPackageNew ########################################### [100%]
|}

In the case above, bMyPackageDep was a prerequisite for aMyPackageNew, so even though the file names sorted in
reverse order, <code>rpm</code> ordered them correctly.

'''rpm -q (query)'''

Several pieces of useful information can be queried from the rpm database. Queries can be run by any user who has
read access to the rpm database. By default, all users have read access. To run a query, use the <code>-q</code> switch with the
name of the package to query. This will return the version of the package.

<code>$ rpm -q MyPackage</code>
<code>MyPackage-1.0.0</code>

The name of the package must be exactly correct. Wild cards are not allowed. However, if you cannot remember the
full name of a package, you can use the <code>grep</code> tool to help find it. Use the <code>-qa</code> switch to query all installed packages
and pipe the information through <code>grep</code> with the text you can remember. For example:

{| class="wikitable"
|-
| <tablestyle="float:right; font-size: 0.9em; width:40%; margin: 0 0 1em 1em;" style="padding:0.5em;">'''The joy of grep'''<<BR>><code>grep</code> is a text search tool that has a wide variety of uses. By default, <code>grep</code> will search files to show you lines that contain the text you indicate. In our example, we searched for "IBM." <code>grep</code> is a powerful tool in your scripting and console work.
|}
<code>$ rpm -qa | grep IBM</code>
<code>IBMWSAppDev-Product-5.0-0</code>
<code>IBMWSSiteDevExp-Core-5.0-0</code>
<code>IBMWSSiteDev-Core-5.0-0</code>
<code>IBMWSTools-WAS-BASE-V5-5.0-0</code>
<code>IBMJava118-SDK-1.1.8-5.0</code>
<code>IBMWSWB-samples-5.0-0</code>
<code>IBMWSWB-5.0-0</code>
<code>IBMWSAppDev-Core-5.0-0</code>
<code>IBMWSAppDev-5.0-0</code>
<code>IBMWSTools-5.0-0</code>

Besides version numbers, <code>rpm -q</code> can provide other useful information about a package. Here are some examples:

'''Getting information with an rpm query'''

{| class="wikitable"
|-
| <code>rpm -q changelog</code>
| Shows the development change history for the package
|-
| <code>rpm -qc</code>
| Shows the configuration files for the package
|-
| <code>rpm -qd</code>
| Shows the documentation files for the package
|-
| <code>rpm -qi</code>
| Shows the package description
|-
| <code>rpm -ql</code>
| Shows a list of the package's files
|-
| <code>rpm -qR</code>
| Shows the dependencies for the package
|}

The query also has another interesting command which is run on files rather than packages.

<code>rpm -q whatprovides <filename></code>

The above command will identify the package that is associated with the filename given. The filename must include
the absolute path to the file, since that is how the information is stored in the rpm database.

----

'''RPM front ends'''

Working with <code>rpm</code> from the console is easy, but sometimes it is more convenient to work with a graphical interface. In
typical Linux style, there are front-end programs which provide an interface to the rpm program. Each distribution has
a front end, which will vary. Consult your distribution documentation for information about the package management
tool provided.

----

'''Webmin software packages'''

Webmin also provides a simple Web-based front end for dealing with RPM packages.

'''Figure 1. Webmin RPM interface'''

<nowiki>[[File:figure11.png]]</nowiki>

Software can be easily installed erased and queried from here. Software can also be installed directly from URL sites.
If you have rpm enhancement tools installed such as apt or the Redhat Network, Webmin will pick them up and
provide an interface to them.

'''Source code'''

Since Linux is an open source operating system, it comes with all of the development tools required to compile
software. While most of the packages that you work with will be provided as binary RPMs, you are not limited to only
those packages. If you wish, you can download the raw source code and custom-compile it for your system.

You should be cautious about compiling from source on a production system as it may cause problems or void your
support for commercial software which you are using on the system, such as IBM DB2. However, being familiar with
compiling from source will allow you to apply patches to software and work with packages ported from other
environments. Once you have compiled the code successfully, it is even possible to create your own RPM!

----

'''Corewars source demonstration'''

To demonstrate how simple it can be to compile from source, we will compile a simulation game called Corewars (see
Resources for a link). Here's a note about Corewars from their Web site: "Corewars is a simulation game where a
number of warriors try to crash each other while they are running in a virtual computer. The warriors can be written in
one of two assembler-like languages called Corewars and Redcode. Corewars is the default language and is easier to
learn and understand. Redcode provides more advanced and powerful instructions but also requires more time to
learn."

The first step to compiling from source is to download the source code package from the Web site:
* http://prdownloads.sourceforge.net/corewars/corewars-0.9.13.tar.gz?download
Once the code is downloaded, I expand the package.

<code>tar -xvzf corewars-0.9.13.tar.gz</code>

The file is expanded into my current directory. The standard approach is for the source code to be contained in a
directory which matches the product name. In this case, it's in a directory called corewars-0.9.13.

I enter into that directory and find the source code, some documentation, configure scripts and README files. Most
source packages will come with a file called INSTALL and one called README. You should read these materials
before you compile the software. They will usually save you a lot of pain by identifying problems before you have
them and advising you of the correct procedures for compiling and installation. Most problems I have had compiling
from source were simply because I didn't follow the directions.

The most common next step is to run the <code>configure</code> script. <code>configure</code> is part of the Autoconf package, included
with the development tools of your Linux distribution. Quoting Autoconf's package description: "GNU's Autoconf is a
tool for configuring source code and Makefiles. Using Autoconf, programmers can create portable and configurable
packages, since the person building the package is allowed to specify various configuration options."

The <code>configure</code> script runs a series of tests on the system to determine the best way to compile the package for your
distribution and architecture. It then createx a custom Makefile for your system. If there are problems with compiling
on your system, <code>configure</code> tells you. <code>configure</code> will usually let you customize the features to be included in the
compile, or let you provide parameters about locations of libraries or other needed files so that the package can be
compiled successfully. Here we execute <code>configure</code> with no additional parameters:

<code>./configure</code>

Several tests run on the system ultimately end with success. Now build the program using:

<code>make</code>

If the compile has errors, I will need to determine the problems and fix them. This can be non-trivial and may require a
good deal of knowledge about your environment and programming in general. If all goes well, we typically install the
software with:

<code>make install</code>

The files are copied into the correct areas of the system, file permissions are updated, configuration files are copied
and documentation is added to the manual pages.
Now let's test our handiwork by executing the program. It is a graphical program, so you will need to have X running
when you start it. The make <code>install</code> which we did above should put the program in our executable path.

<code>corewars</code>

A graphical screen should appear to reward us.

'''Figure 2. Success!'''

<nowiki>[[File:figure12.png]]</nowiki>

The topic of corewars rules is outside of the scope of this article, but you will find documentation about this interesting
simulation game in the man page (<code>man corewars</code>).

The corewars compile was a typical scenario. There are many possible variations including using switches on the
configure script to adjust the features that are compiled into the program, using different commands from the
Makefile to adjust how the compile is done, and others.

Since this program was not installed using rpm, there are no entries in the rpm database. If a program doesn't work out
after it's been installed, most Makefiles include an uninstall parameter to remove the software:

<code>make uninstall</code>

Bear in mind that working with raw source code does not enter anything into the RPM database. Software installed in
this way is unmanaged, so it should be done with care.

'''Source RPMs'''

When an RPM is created, there is an artifact called a Source RPM. This is a SPEC file combined with source code
designed to build on one or more architectures. This is the best of both worlds! With a source RPM, you can custom
compile the software on your system, but the finished product will be an installable RPM rather than the raw binaries.
Most packages that are available as a pre-compiled RPM are also available as a SRPM. This can be a simple way to
move software across platforms in Linux. When you have success recompiling onto a different platform, consider
sharing your finished RPMs with the community.

----

'''May the source be with you'''

If you are new to Linux, installing software has a different approach than what you are used to. However, the RPM
approach to installation is elegant and provides new power which you will soon grow to appreciate.

You should become familiar with the options for working with rpms from the console, but for daily use there are
front-end interface options to make rpms easier to manage. One was provided by your distribution, and others are
available, such as the one in Webmin.

You are not limited by pre-compiled rpms, though. You can take advantage of the open source nature of Linux to
compile applications directly from the source code. Compiling is generally easy for a mature project. Remember that
code installed from source code will not have an entry in your rpm database. When working with source, consider
using source rpms, which combine the power of compiled source with the manageability of rpms.

----

'''Resources'''

* Check out the other parts in the Windows-to-Linux roadmap series (developerWorks, November 2003). 
* The IBM developerWorks tutorials LPI certification exam prep are very useful. Part 1 covers Red Hat and Debian package managers as well ascompiling programs from sources and managing shared libraries. Part 2 covers important kernel configuration and in-kernel PCI and USB support.
* The IBM developerWorks tutorial Compiling and installing software from sources goes into more depth on unpacking, inspecting, configuring and installing software packages from source code.
* Learn more about compiling and installing source code on Linux with the IBM developerWorks feature, Compiling and installing software from sources .
* Learn how to create your own packages with the IBM developerWorks features Create Debian Linux packages and Packaging software with RPM.
* Learn about an alternative to RPM and Debian's package management solutions in the IBM developerWorks feature Manage packages using Stow.
* Regularly upgrading software is an important part of security. Learn how to keep your system up to date quickly and easily with the IBM developerWorks Tip on Upgrading applications from sources.
* Learn more about United Linux in this IBM developerWorks feature. SourceForge is an excellent resource for open source projects; you can add your own projects as well.
* The Corewars project is just one of many posted at SourceForge.
* The developerWorks Open source zone hosts open-source projects at IBM.
* IBM's alphaWorks offers early access to emerging IBM technologies including things like the Web services and Grid toolboxes.
* The IBM Speed-start your Linux app 2003 initiative offers a free Linux Software Evaluation Kit (SEK) which includes DB2 Universal Database, WebSphere Application Server, WebSphere Studio Site Developer, WebSphere MQ, Lotus Domino, Tivoli Access Manager -- and more!

----

'''About the author'''

Chris Walden is an e-business Architect for IBM Developer Relations Technical Consulting in Austin, Texas,
providing education, enablement, and consulting to IBM Business Partners. He is the official Linux fanatic on his
hallway and does his best to spread the good news to all who will hear it. In addition to his architect duties, he
manages the area's all-Linux infrastructure servers, which include file, print, and other application services in a
mixed-platform user environment. Chris has ten years of experience in the computer industry ranging from field
support to Web application development and consulting.
