{|
| '''Warning'''
* This is a '''readonly''' and '''text-based''' archive of a deprecated wiki.
* '''Images''' and '''attachments''' have been removed to conserve space.
* '''Links''' may not work and there may be formatting issues.
* A '''compressed''' version with images and the original syntax is in the repo '''Releases'''.
|}

__TOC__

For the sample configuration we will have two nodes called ''ha01'' and ''ha02'', and we will service an IP address, that we will call the Virtual IP address (VIP), on an '''active / passive''' configuration. Both nodes will need to have name resolution configured either through DNS or the ''/etc/hosts'' file. 

Heartbeat V2 style config has 3 main configuration files. These are /etc/ha.d/authkeys, /etc/ha.d/ha.cf, and /var/lib/heartbeat/crm/cib.xml. There is a fourth configuration file, /etc/logd.cf, which contains the log settings.

* On the first host, edit /etc/ha.d/authekys. This file is the same in all Heartbeat style configurations. It stores the Heartbeat authentication information that will be used in the cluster nodes.

<pre>
auth 1
1 md5 DesiredPassword
</pre>

* Then, change permissions of the file:

<pre>
sudo chmod 600 /etc/ha.d/authkeys
</pre>

* Then, create the /etc/logd.cf. 

<pre>
debugfile /var/log/ha-debug
logfile /var/log/ha-log
</pre>

* Then, edit /etc/ha.d/ha.cf with the following:

<pre>
use_logd on
udpport 694
keepalive 2
warntime 15
deadtime 30
initdead 30
mcast eth0 239.0.0.43 694 1 0
node ha01 ha02
crm yes
</pre>

* To finish, copy all configuration files to the second node:

<pre>
scp /etc/ha.d/authkeys ha02:~
scp /etc/ha.d/ha.cf ha02:~
scp /etc/logd.cf ha02:~
</pre>

* And move then to the corresponding directories in the second node.

<pre>
sudo mv authkeys /etc/ha.d/
sudo mv ha.cf /etc/ha.d/
sudo mv logd.cf /etc/
</pre>

* Then, start Heartbeat and wait for it to make intra cluster communication so that we can add the service to the cluster resources manager.

<pre>
sudo /etc/init.d/heartbeat start
</pre>

* Once it has made intra cluster configuration, specify some global options for the cluster resource manager:

<pre>
crm_attribute --attr-name symmetric-cluster --attr-value true
crm_attribute --attr-name default-resource-stickiness --attr-value INFINITY
</pre>

* Then, edit the following file /tmp/resources.xml, which configuration parameters for the resource:

<pre>
	<group id="group_1">
         <primitive class="ocf" id="VIP_eth0" provider="heartbeat" type="IPaddr2">
           <operations>
             <op id="VIP_eth0_mon" interval="5s" name="monitor" timeout="5s"/>
           </operations>
           <instance_attributes id="VIP_eth2_inst_attr">
             <attributes>
               <nvpair id="VIP_eth0_attr_0" name="ip" value="192.168.0.100"/>
               <nvpair id="VIP_eth0_attr_1" name="netmask" value="24"/>
               <nvpair id="VIP_eth0_attr_2" name="nic" value="eth0"/>
             </attributes>
           </instance_attributes>
         </primitive>
       	</group>
</pre>

* After creating above file, you need to tell Heartbeat to add it to the cluster configuration.

<pre>
cibadmin -o resources -C -x /tmp/resources.xml
</pre>

* Finally, edit the following file, /tmp/constraints.xml, which will specify the constraints, that will tell the Cluster Resource Manager how it should failover the services.

<pre>
	<constraints>
       	 <rsc_location id="rsc_location_group_1" rsc="group_1">
          <rule id="prefered_location_group_1" score="100">
           <expression attribute="#uname" id="prefered_location_group_1_expr" operation="eq" value="ha01"/>
          </rule>
       	 </rsc_location>
	</constraints>
</pre>

After editing above file, you need to tell Heartbeat to add it to the cluster configuration.

<pre>
cibadmin -o constraints -C -x /tmp/resources.xml
</pre>
