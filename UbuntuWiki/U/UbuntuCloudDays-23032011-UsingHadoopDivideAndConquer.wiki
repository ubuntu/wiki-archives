{|
| '''Warning'''
* This is a '''readonly''' and '''text-based''' archive of a deprecated wiki.
* '''Images''' and '''attachments''' have been removed to conserve space.
* '''Links''' may not work.
* A '''full compressed version''' of the wiki is available on archive.org
|}

__TOC__

=== Using hadoop, divide and conquer -- edulix ===

<pre>
[18:56] <kim0> Next up is Edulix .. Presenting "Hadoop" The ultimate hammer to bang on big data :)
[18:56] <ClassBot> There are 5 minutes remaining in the current session.
[19:00] <Edulix> hello people, thanks for your assistance. this is the session titled "Using hadoop, divide and conquer"
[19:01] <Edulix> kim0 told me about these ubuntu cloud sessions, and kidly asked me to do a talk over hadoop, so here I am  =)
=== ChanServ changed the topic of #ubuntu-classroom to: Welcome to the Ubuntu Classroom - https://wiki.ubuntu.com/Classroom || Support in #ubuntu || Upcoming Schedule: http://is.gd/8rtIi || Questions in #ubuntu-classroom-chat || Event: Ubuntu Cloud Days - Current Session: Using hadoop, divide and conquer - Instructors: edulix
[19:01] <Edulix> first I must say that I am in no way a hadoop expert, as I have been working with hadoop just for a bit over a month
[19:01] <ClassBot> Logs for this session will be available at http://irclogs.ubuntu.com/2011/03/24/%23ubuntu-classroom.html following the conclusion of the session.
[19:02] <Edulix> but I hope that I can help to show you a bit of hadoop and ease the learning curve for those who want to use it
[19:03] <Edulix> I'm going to base this talk in the hadoop tutorial available in http://developer.yahoo.com/hadoop/tutorial/ as it helped me a lot, but it's a bit dense, so I'll do a resumed version
[19:03] <Edulix> So what's hadoop anyway?
[19:03] <Edulix> it's a large-scale distributed batch processing infrastructure, designed to efficiently distribute large amounts of work across a set of machines
[19:03] <Edulix> here large amounts of work means really really large
[19:04] <Edulix> Hundreds of gigabytes of data is low end for hadoop!
[19:04] <Edulix> hadoop supports handling hundreds of petabytes... Normally the input data is not that big, but the intermediate data is or can be
[19:04] <Edulix> of course, all this does not fit on a single hard drive, much less in memory
[19:05] <Edulix> so hadoop comes with support for its own distributed file system: HDFS
[19:05] <Edulix> which breaks up input data and sends fractions  (blocks) of the original data to some machines in your cluster
[19:06] <Edulix> everyone that has tried will know that performing large-scale computation is difficult
[19:06] <Edulix> whenever multiple machines are used in cooperation with one another, the probability of failures rises: partial failures are an expected and common
[19:07] <Edulix> Network failures, computers over heating, disks crashing, data corruption, maliciously modified data..
[19:07] <Edulix> shit happens, all the time (TM)
[19:07] <Edulix> In all these cases, the rest of the distributed system should be able to recover and continue to make progress. the show must go on
[19:08] <Edulix> Hadoop provides no security, and no defense to man in the middle attacks for example
[19:08] <Edulix> it assumes you control your computers so they are secure
[19:08] <Edulix> on the other hand, it is designed to handle hardware failure and data congestion issues very robustly
[19:09] <Edulix> to be successful, a large-scale distributed system must be able to manage resources efficiently
[19:09] <Edulix> CPU, RAM, Harddisk space, network bandwidth
[19:10] <Edulix> This includes allocating some of these resources toward maintaining the system as a whole
[19:10] <Edulix> ..... while devoting as much time as possible to the actual core computation
[19:10] <Edulix> So let's talk about the hadoop approach to things
[19:11] <Edulix> btw if you have nay questions, just ask in #ubuntu-classroom-chat with QUESTION: your question
[19:11] <Edulix> Hadoop uses a  simplified programming model which allows the user to quickly write and test distributed systems
[19:12] <Edulix> and also to tests its efficient & automatic distribution of data and work across machines
[19:13] <Edulix> and also allows to use the underlying parallelism of the CPU cores
[19:13] <Edulix> In a hadoop cluster, data is distributed to all the nodes of the cluster as it is being loaded in
[19:13] <Edulix> HDFS will split large data files into blocks which are managed by different nodes in the cluster
[19:13] <Edulix> Also replicating data in different nodes, just in case
[19:14] <ClassBot> kim0 asked: Does hadoop require certain "problems" that fits its model ? can I throw random computations to it
[19:15] <Edulix> I'm going to answer that now =)
[19:16] <Edulix> basically, hadoop uses the mapreduce programming paradigm
[19:16] <Edulix> In hadoop, Data is conceptually record-oriented. Input files are split into input splits referring to a set of records.
[19:17] <Edulix> The stragy of the scheduler is moving the computation to the data, i.e. which data will be processed by a node is chosen based on its locality to the node, which results in high performance.
[19:17] <Edulix> Hadoop programs need to follow a particular programming model (MapReduce), which limits the amount of communication, as each individual record is processed by a task in isolation from one another
[19:18] <Edulix> In MapReduce, records are processed in isolation by tasks called Mappers
[19:18] <Edulix> The output from the Mappers is then brought together into a second set of tasks called Reducers
[19:18] <Edulix> where results from different mappers can be merged together
[19:18] <Edulix> Note that if you for example don't need the Reduce step, you can implement a Map-only processing.
[19:19] <Edulix> This simplification makes the Hadoop framework much more reliable, because if a node is slow or crashes, other node can simply replace the former taking the same inputsplit and processing it again
[19:19] <ClassBot> chadadavis asked: Is there any facility for automatically determining how to partition the data, i.e. based on how long one chunk of processing takes?
[19:21] <Edulix> to be able to partitoon the data,
[19:21] <Edulix> you need to have first a structure for that data. for example,
[19:22] <Edulix> if you have a png image that you need processthen the input data is the image file. you might partition your image in chunks that start in a given position (x,y) and have a height and a width
[19:22] <Edulix> but the partitioning is usually done by you, the hadoop program developer
[19:23] <Edulix> though hadoop is in charge of selecting where to send to that partition, depending on data locality
[19:24] <Edulix> when you partition the input data, you don't send the data (input split) to the node that will process it: ideally it will already have that data!
[19:24] <Edulix> how is this possible?
[19:25] <Edulix> because when you do the partition, the InputSplit only defines this partition (so it might be in the image example 4 numbers: x,y, height, width) and depending on which nodes the file blocks of the input data reside, hadoop will send that split to that node
[19:26] <Edulix> and then the node will open the file in HDFS for reading starting (fseek) in there
[19:26] <Edulix> ok, I continue =)
[19:26] <Edulix> separate nodes in a Hadoop cluster still communicate with one another, implicitly
[19:27] <Edulix> pieces of data can be tagged with key names which inform Hadoop how to send related bits of information to a common destination node
[19:27] <Edulix> Hadoop internally manages all of the data transfer and cluster topology issues
[19:27] <Edulix> One of the major benefits of using Hadoop in contrast to other distributed systems is its flat scalability curve
[19:28] <Edulix> Using other distributed programming paradigms, you might get better results for 2, 5, perhaps a dozen machines. But when you need to go really large scale, this is where hadoop excels
[19:29] <Edulix> After you program is written and functioning on perhaps ten nodes (to tests that it can be used in multiple nodes with replication etc and not only in standalone mode),
[19:29] <Edulix> then  very little --if any-- work is required for that same program to run on a much larger amount of hardware efficiently
[19:29] <Edulix> == distributed file system ==
[19:29] <Edulix> a distributed file system is designed to hold a large amount of data and provide access to this data to many clients distributed across a network
[19:30] <Edulix> HDFS is designed to store a very large amount of information, across multiple machines, and also supports very large files
[19:30] <Edulix> some of its requirements are:
[19:30] <Edulix> it should store data reliably even if some machines fail
[19:30] <Edulix> it should provide fast, scalable access to this information
[19:31] <Edulix> And finally it should integrate well with Hadoop MapReduce, allowing data to be read and computed upon locally when possible
[19:31] <Edulix> This last point is crucial. HDFS is optimized for MapReduce, and thus has made some decisions/tradeoffs:
[19:31] <Edulix> Applications that use HDFS are assumed to perform long sequential streaming reads from file because of MapReduce
[19:31] <Edulix> so HDFS is optimized to provide streaming read performance
[19:32] <Edulix> this comes at the expense of random seek times to arbitrary positions in fileswhen a node
[19:32] <Edulix> this comes at the expense of random seek times to arbitrary positions in files
[19:32] <Edulix> i.e. when a node reads, it might start reading in the middle of a file, but then it will read byte after byte, not jumping here and there
[19:32] <Edulix> Data will be written to the HDFS once and then read several times; AFAIK there is no file update support
[19:33] <Edulix> due to the large size of files, and the sequential nature of reads, the system does not provide a mechanism for local caching of data
[19:33] <Edulix> data replication strategies combat machines or even whole racks failing
[19:34] <Edulix> hadoop comes configured to have each file block stored in three nodes by default: two in the same rack, and the other block in another machine
[19:35] <Edulix> if the first rack fails, speed might degrade relatively but information wouldn't be lost
[19:35] <Edulix> BTW HDFS design is based on google file system (GFS)
[19:36] <Edulix> and as you probably  has guessed, in HDFS data/files is/are split in blocks of equal size in DataNodes (machines in the cluster)
[19:36] <ClassBot> gaberlunzie asked: would this sequential access mean hadoop can work with tape?
[19:37] <Edulix> I haven't heard anyone doing such a thing,
[19:37] <Edulix> and I don't think it's a good idea
[19:38] <Edulix> why? because the reads are sequential, but you need to do the first seek to start reading at the point your inputsplit indicates
[19:38] <Edulix> doing this first seek might be too slow for a tape, but I might be completely wrong  here
[19:39] <Edulix> note that the data stored in HDFS is supposed to be temporary, mostly, just for working
[19:39] <Edulix> so you copy the data there, do your thing, then copy the output result back
[19:39] <Edulix> in contrast, tapes are mostly used for large term storage
[19:40] <Edulix> (cotinuing) default block size in HDFS is very large (64MB)
[19:40] <Edulix> This decreases metadata overhead and allows for fast streaming reads of data
[19:41] <Edulix> Because HDFS stores files as a set of large blocks across several machines, these files are not part of the ordinary file system
[19:41] <Edulix> For each DataNode machine, the blocks it stores reside in a particular directory managed by the DataNode service, and these blocks are stored as files whose filenames are their blockid
[19:41] <Edulix> HDFS comes with its own utilities for file management equivalent to ls, cp, mv, rm, etc
[19:41] <Edulix> the metadata (names of files and dirs and where are the blocks stored) of the files can be modified by multiple clients concurrently
[19:42] <Edulix> The metadata (names of files and dirs and where are the blocks stored) of the files can be modified by multiple clients concurrently. To orchestrate this, metadata is stored and handled by the NameNode, that stores metadata usually in memory (it's not much data), so that it's fast (because this data *will* be accessed randomly).
[19:43] <ClassBot> chadadavis asked: If I first have to copy the data (e.g. from a DB) to HDFS before splitting, couldn't the mappers just pull/query the data directly from the DB as well?
[19:43] <Edulix> yes you can =)
[19:43] <Edulix> and if the data is in a DB, you should
[19:45] <Edulix> input data is read from an InputFormat
[19:45] <Edulix> and there are different input formats provided by hadoop: FileInputFormat for example to read from a single file
[19:45] <Edulix> but there's also DBInputFormat, for example
[19:46] <Edulix> in my experience, you will probably create your own =)
[19:46] <Edulix> Deliveratively I haven't explained any code, but I recommend you that if you're interested you should start playing with hadoop locally in your own machine
[19:47] <Edulix> just download hadoop from http://hadoop.apache.org/ and follow the quickstart http://hadoop.apache.org/common/docs/r0.20.2/quickstart.html
[19:47] <Edulix> for quickstart and for development, you typically use hadoop as standalone in your own machine
[19:47] <Edulix> in this case HDFS will simply refer to your own file system
[19:47] <Edulix> You just need to download hadoop, configure Java (because hadoop is written in java), and execute the example as mentioned in the quickstart page
[19:47] <Edulix> as mentioned earlier, with hadoop you usually operate as follows, because of its batching nature: you copy input data to HDFS, then request to launch the hadoop task with an output dir, and when it's done, the output dir will have the task results
[19:48] <Edulix> For starting developing a hadoop app was this tutorial because it explains pretty much everything I needed http://codedemigod.com/blog/?p=120
[19:48] <Edulix> but note that it's a bit old
[19:48] <Edulix> and one of the things that I found most frustrating in hadoop while developing was that there are duplicated classes i.e. org.apache.hadoop.mapreduce.Job and org.apache.hadoop.mapre.jobcontrol.Job
[19:49] <Edulix> In that case, use alwys org.apache.hadoop.mapreduce, because is the new improved API
[19:49] <Edulix> be warned, the examples in http://codedemigod.com/blog/?p=120 use the old mapred api :P
[19:50] <Edulix> and hey, now I'm open to even more questions !
[19:51] <Edulix> and if you have questions later on, you can always join us in freenode.net, #hadoop, and hopefully someone will help you there =)
[19:51] <ClassBot> There are 10 minutes remaining in the current session.
[19:52] <ClassBot> gaberlunzie asked: does hadoop have formats to read video (e.g., EDLs and AAFs)?
[19:52] <Edulix> most probably.. not, but maybe someone has done that before
[19:52] <Edulix> anyway, creating a new input format is really easy
[19:53] <ClassBot> chadadavis asked: Mappers can presumably also be written in something other than Java? Are there APIs for other languages (e.g. Python?) Or is managed primarily at the shell level?
[19:54] <Edulix> good question!
[19:54] <Edulix> yes, there are examples in python and in C++
[19:55] <Edulix> I haven't used them though
[19:55] <ClassBot> kim0 asked: Can I use hadoop to crunch lots of data running on Amazon EC2 cloud ?
[19:55] <Edulix> heh I forgot to mention it =)
[19:56] <Edulix> answer is yes!
[19:56] <Edulix> more details in http://aws.amazon.com/es/elasticmapreduce/
[19:56] <ClassBot> There are 5 minutes remaining in the current session.
[19:56] <Edulix> that's one of the nice things of using hadoop: many big people uses it in the industry. yahoo, for example, and amazon has support for it too
[19:57] <Edulix> so don't need to really have lots of machines for doing large computation
[19:57] <Edulix> just use amazon =)
[19:57] <ClassBot> gaberlunzie asked: is there a hadoop format repository?
[19:58] <Edulix> I don't know huh
[19:58] <Edulix> :P
[19:58] <Edulix> I didn't investigate much about this because I needed to have my own
[19:58] <Edulix> but probably in contrib there is
[20:00] <Edulix> ok so that's it!
[20:00] <Edulix> Thanks for your assistance to the talk, and thanks for the organizers
=== ChanServ changed the topic of #ubuntu-classroom to: Welcome to the Ubuntu Classroom - https://wiki.ubuntu.com/Classroom || Support in #ubuntu || Upcoming Schedule: http://is.gd/8rtIi || Questions in #ubuntu-classroom-chat || Event: Ubuntu Cloud  Days - Current Session: UEC/Eucalyptus Private Cloud - Instructors: obino
[20:01]  * kim0 claps .. Thanks Edulix 
</pre>
