{|
| '''Warning'''
* This is a '''readonly''' and '''text-based''' archive of a deprecated wiki.
* '''Images''' and '''attachments''' have been removed to conserve space.
* '''Links''' may not work.
* A '''full compressed version''' of the wiki is available on archive.org
|}

__TOC__

=== What we did ===

==== Smoke testing / dashboard ====
* <nowiki>[[https://blueprints.launchpad.net/ubuntu/+spec/other-p-builds-smoke-testing|Identify build breaks on daily ISOs]]</nowiki>: start tracking the quality of the builds, define what a broken build is and track how long it takes to engineering teams to fix the problems they introduce that impact testing. By doing this, the aim is to raise awareness of impact of untested submissions and to be able to determine how many times we are held by bad submissions. 
* Have a metric to be able to know how many days on a cycle we have a working ISO. 

   '''Ideas'''
** ISOs should be validated taking into account md5 signature, packages in that ISO match the manifest, ISO tree, etc. This test case should be executed before the default installation test. Check that runtime generated files are not there. 
** After ISO static validation, we run default installation.
** After the system is installed, we run basic test cases on all the main components. 
** Parse the installation log and check for warnings and problems during the install (add remarks in case something has gone wrong during the installation).
 

   '''Actions'''
** Decide which information should be in the report: 
**** <nowiki>[[https://wiki.ubuntu.com/QATeam/AutomatedTesting/Strategy?action=AttachFile&do=get&target=dashboard.html|Dashboard]]</nowiki>: build number, type of build, release, total tests, tests run, tests passed, pass rate and color classification (red, amber, green), defects found during smoke testing and comments related to the areas where those defects are, does the image install properly
**** Build page: link to iso file, build logs, test cases breakdown (unit tests, smoke tests), list of bugs generated and status at the time of loading the page, total time that took to fix the defect when the defect gets fixed.  
** Decide the testing that we are going to do to decide whether a product build is broken or worth for further testing
** Create a website to keep track of product builds sanity.
** Keep a history of builds that we can refer to in the future to verify defects and for any testing reason (identify regressions).
** Set up a mailing for people to be able to subscribe to the build status (try to structure in a way people do not have to read all the email but just the bits they are interested in). 
** '''Objective''': Determine if a build is worth using for further testing.
** '''Who''': QA Team
** '''Size''': 3 months

* (This task will be discussed also as part of the previous <nowiki>[[https://blueprints.launchpad.net/ubuntu/+spec/other-p-builds-smoke-testing|blueprint]]</nowiki>). Add a dashboard to have a consolidated view of unit testing status. Collect initiatives from different engineering teams and either add it to Jenkins or consolidate it to make the data accessible and usable to track quality of the submissions.  
   '''Actions'''
** Figure out where all the logs are (builder, launchpad)
** Write a script to extract the data
** Find the right tool to analyze and show the results
** '''Who''': QA Team
** '''Size''': 2 month

==== Summary of achievements on smoke testing / dashboard ====

During Precise we have put together a <nowiki>[[https://jenkins.qa.ubuntu.com/view/Precise%20ISO%20Testing%20Dashboard/view/Daily/?|dashboard]]</nowiki> that summarizes the results for the currently run tests and have put together a bug report that helps track bugs found with automated testing.

The smoke testing have been running consistently daily during Precise. More tests need to be added to improve the coverage, but one big problem needed to be overcome first, being able to run more than one test case per job in Jenkins was a must. For this reason amongst others, a new test harness is being put in place to aid test suites to run on a particular installation of Ubuntu in one single run, this test harness won't be limited to smoke testing, which will be one test suite amongst many, it will define an easy and handy way to add automated test cases.  

==== Test case management ====

* <nowiki>[[https://blueprints.launchpad.net/ubuntu/+spec/other-p-qa-test-case-management-tool|Piloting a new test case management tool]]</nowiki>. Find an appropriate test case management tool and install it, find a way to integrate it with http://testcases.qa.ubuntu.com/. 1 cycle should be enough to install the tool and get the existing test cases that is worth keeping in there, we can also pilot the usage of it within QA before we move to the community using it. 
**** Patrick has tested testlink (http://www.teamst.org/) during Natty and built a prototype.
**** Another option is the Litmus test case management tool, MPL license, <nowiki>[[https://wiki.mozilla.org/Litmus|https://wiki.mozilla.org/Litmus]]</nowiki>. Just by registering on the web and doing some basic mozilla testing it strikes as a very adequate tool for the job. 
**** We have tried <nowiki>[[https://wiki.mozilla.org/Testopia|testopia]]</nowiki>, and it is a very unfriendly and unusable tool, the frontend of Litmus looks much better. 
****** Requirements for the tool:
******* Allow separation of manual and automated testing
******* Allow the addition and review of new test cases
******* Test case execution and easy/consistent results reporting
******* Support several testing configurations and keep track of the tester's environment/config
    '''Actions'''
*** Decide which tool to use 
*** Install it
*** Writing a script to export our current test cases/metadata into the tool
*** Get the pilot up and running and some people using it (feedback on missing/wrong things)
*** Establish new test cases review/creation good practices and get buying in from the community, probably by sending them for review and agreeing on something suitable
*** Correct the problems
*** '''Who''': QA Team + QA Community Coordinator
*** '''Size''': 3 months

===== Summary of achievements on Test case management =====
During Q it became evident that having one single place where to store test cases may be impractical in the long run. A test case management system is still necessary, but it is more important to have an organized way of adding automated test cases and manual test cases, even if that means having the test cases spread across different sub-systems or bzr branches.

Case Conductor was chosen as a tool for this pilot, because it was the successor of litmus. The tool is still on its early stages and hasn't been fully released by the end of Precise. We will be making sure test cases are properly documented and easily accessible/modifiable, but initially we won't use a single tool for automated and manual testing. 

Documentation is being put in place to help new contributors and existing ones deal with test cases, as well as adding new ones when that becomes necessary. Manual testing will be performed using checkbox that is being pushed forward by Ubuntu QA and automated testing will be done using a common approach to automation defined by the Platform QA team, it will be open for review and modification ideally by UDS-Q. 

A single way of reporting results is being put in place, so that results from manual and automated testing can be reported and considered jointly to help make the decision process easier.

==== Metrics ====

* <nowiki>[[https://blueprints.launchpad.net/ubuntu/+spec/other-p-qa-metrics|Quality Metrics]]</nowiki>. Decide which metrics are the most appropriate to start with and start tracking them, publish results to management and the community to gain momentum and enable people to see what their contributions actually contribute to the overall quality of Ubuntu
*** Coverage (conditional, functional)
***** The kernel has been instrumented with '''gcov''' before, some example results to be found here: <nowiki>[[http://reports.qa.ubuntu.com/reports/ogasawara/gcov-isos/|http://reports.qa.ubuntu.com/reports/ogasawara/gcov-isos/]]</nowiki>
*** Defects
***** Define a test escape analysis strategy during this cycle. 

   '''Actions'''
** Investigating how to instrument Ubuntu (for coverage) and decide which tools are the most appropriate
** Set up the web/dashboard for publishing results
** Determine what packages to instrument
** Determine which information is required for defect analysis and convince the bug-control team to provide us with it
** '''Who''': QA Team
** '''Size''': 4 months

===== Summary of achievements on Metrics =====
The dashboards for automated testing and open bugs report associated with them are the main metric that has been established during Precise. 

Some development teams are experimenting with gcov and are in the process of adding hooks to their makefiles to be able to generate binaries that can be used to measure coverage, however no ISO has been built jointly for this purpose. Ubuntu is getting slowly ready for this, but some integration efforts across different packages need to be put in place. Coverage will be measured to start with (during Q) based on what packages have test cases associated with them and running regularly.  

==== QA Community ====
* Create a launchpad QA group similar to the Bug Control group. People willing to do testing will demonstrate their ability to create/correct test cases and will earn their tester status through good work, this way we ensure that people in the group are committed to do good testing and know how to do it.
   '''Actions'''
** Create launchpad group and do some marketing work to it
** Create a wiki with the rules to become part of the group and instructions on how to create good test cases. (http://qa.ubuntu.com)
** Post information about what we are doing and get people excited about it
** '''Who''': QA Community Coordinator
** '''Size''': 6 weeks

* Testing of the P release as we've done previously
** '''Size''': 6 months duration

* Rewriting of the QA wiki: Define the role of the different players in the wiki, so that everyone knows what to expect in relation to QA processes. 

===== Summary of achievements on QA Community =====
The existing manual test cases were mostly rewritten and are being used in a new fashion with checkbox during beta1 and beta2. Consolidation and reporting of results is still an issue that is being worked out. 

A proposal for consolidating the QA community is in place and will be discussed during UDS-Q. 
