{|
| '''Warning'''
* This is a '''readonly''' and '''text-based''' archive of a deprecated wiki.
* '''Images''' and '''attachments''' have been removed to conserve space.
* '''Links''' may not work.
* A '''full compressed version''' of the wiki is available on archive.org
|}

__TOC__


* '''Launchpad Entry''': UbuntuSpec:hardware-certification-kvm-access
* '''Created''': 2008-12-15
* '''Contributors''': cr3

=== Summary ===

The Hardware Certification department consists of a lab with laptops, a colocation center with desktops and eventually the London datacenter with servers. We need to find a way for the QA and distro teams to have a practical access to these physical resources. This means knowing which machines are available, which components correspond to which machines and where these machines are located.

=== Rationale ===

In debugging hardware issues it helps to have direct access to the machine.

=== Use Cases ===

* Steve is working on a hardware-specific SRU verification and has found that two machines in the lab have the device listed in the bug. He has full access to the testing infrastructure and is therefore able to set up a test environment for himself by kicking off a boot and install sequence on the two machines. 
* Andy is a kernel developer who wants to stress test a bug fix for a disk controller bug on 4 machines in the lab. He contacts Steve who has access to set up a test environment for him. Once the systems are booted and installed Andy can log in via KVM and run his tests.

=== Design ===

A team of people will be set up who have access to the testing infrastructure back-end and will be able to set up test environments for others. Initial members: MarcTardif, DavidMurphy, SteveBeattie, BrianMurray and LeannOgasawara.

Documentation will be written to enable the above team to set up test systems and other platform team members to access the systems.

A regularly update tarball with machine HW log files will be made available that can be searched locally to identify machines for testing.

Due to the commercial nature of the certification program the testing infrastructure will only be available to Canonical employees at this time.

==== Future work ====

In a second development phase we will make it possible to search for and enqueue a system for testing via the certification website. Targeted for Jaunty+1.

=== Implementation ===

* Identify the access levels required and file RT tickets
** nickel and servernab
* Follow up at the February sprint to ensure everyone indeed has access

==== Code Changes ====

None. '''(is this correct? will providing access and documentation be enough for this phase or will changes be needed to the satellite servers or certification site? -- heno)'''

=== Test/Demo Plan ===

This works if individual platform team members are able to contact a member of the infrastructure team to have a system set up and can access it without significant blockers.

=== BoF agenda and discussion ===

This means knowing which machines are available, which components correspond to which machines and where these machines are located.

3 locations that contain hardware
 - London / servers and serial console access (command line interface)
  - nickel.ubuntu.com
 - Montreal (desktops)
  - kvm access (full desktop environment)
 - Montreal lab (laptops) (command line interface)
  - ssh access only

Goal of providing platform team access to the hardware
 - being able to search for hardware to connect to (useful but hard?) possible to search for pci-id(s)
 - contact cr3 or schwuk and they can search for machines
  - contact how? (via e-mail or irc)
 - each machine has a gateway that is necessary to connect to so one can access the system
  - servernab group is for the London datacenter
 - need to ensure that the machine you want to test has the right release on it
  - useful for platform team members to enqueue a release for installing on the target system
   - contact cr3 for getting a release installed on any system
   - medium term solution of having QA team members to be able install releases on the machines
    - people in this team should be across multiple timezones (ogasawara and ara)
 - machines should be blocked from other tasks (installations)
  - have a checkbox that indicates that it is being actively used and timeout after (48 hours?)

Process:
 - determine if hardware is available (wait ~5 minutes)
 - get machine installed (wait ~60 minutes)

Possible use cases:
  - load testing network adapter(s) try using internal mirrors for cd image
   - speed and duplex connection testing -> should happen in checkbox
  - wireless testing - any infrastructure? no
  - suspend and resume testing -> should happen in checkbox

----
CategorySpec
